#!/bin/bash

# BLIP2å¢å¼ºæ•°æ®MGM-2B LoRAè®­ç»ƒè„šæœ¬
# ä½¿ç”¨17,509æ¡BLIP2å¢å¼ºé«˜è´¨é‡æ•°æ®è¿›è¡Œè®­ç»ƒ
# åŸºäºä¹‹å‰çš„LoRAæˆåŠŸç»éªŒï¼Œä¼˜åŒ–å†…å­˜ä½¿ç”¨

# ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„Pythonç¯å¢ƒ
export PYTHONPATH="$(cd "$(dirname "$0")" && pwd)/training:$PYTHONPATH"
export CUDA_VISIBLE_DEVICES=0
# ç¦ç”¨DeepSpeed CUDAæ‰©å±•ç¼–è¯‘ä»¥é¿å…ç‰ˆæœ¬ä¸åŒ¹é…
export DS_BUILD_OPS=0
export DS_SKIP_CUDA_CHECK=1
# è®¾ç½®CUDA_HOMEä»¥é¿å…CUDAç›¸å…³é”™è¯¯
export CUDA_HOME=/home/robot/lhp/miniconda3/envs/Syn0625
# è®¾ç½®PyTorchå†…å­˜ç®¡ç†ä»¥å‡å°‘å†…å­˜ç¢ç‰‡
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

############################################################################
########################### Editable Part Begins ###########################
############################################################################
# exp meta information
EXP_NAME=blip2-enhanced-lora

# training args - åŸºäºBLIP2å¢å¼ºæ•°æ®ä¼˜åŒ–
# pretraining - ä½¿ç”¨ä¿å®ˆå‚æ•°ç¡®ä¿ç¨³å®šæ€§
# make sure PRETRAIN_BATCH_SIZE_PER_GPU * PRETRAIN_GRADIENT_ACCUMULATION_STEPS * num_gpus = 256
PRETRAIN_BATCH_SIZE_PER_GPU=1  # å‡å°æ‰¹æ¬¡å¤§å°é¿å…OOM
PRETRAIN_GRADIENT_ACCUMULATION_STEPS=256  # å¢åŠ æ¢¯åº¦ç´¯ç§¯ä¿æŒå…¨å±€æ‰¹æ¬¡å¤§å°
PRETRAIN_DATALOADER_NUM_WORKERS=1  # å‡å°‘workeré¿å…å†…å­˜é—®é¢˜

# finetuning - ä¿æŒæ•™å¸ˆè¦æ±‚çš„æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
# make sure FINETUNE_BATCH_SIZE_PER_GPU * FINETUNE_GRADIENT_ACCUMULATION_STEPS * num_gpus = 128
FINETUNE_BATCH_SIZE_PER_GPU=1  # æœ€å°æ‰¹æ¬¡å¤§å°
FINETUNE_GRADIENT_ACCUMULATION_STEPS=128  # æ•™å¸ˆè¦æ±‚ï¼Œä¸èƒ½ä¿®æ”¹
FINETUNE_DATALOADER_NUM_WORKERS=1  # æœ€å°workeræ•°

# log and ckpt
LOGGING_STEP=10  # å¢åŠ æ—¥å¿—é—´éš”å‡å°‘I/O
CKPT_SAVE_STEPS=500  # å¢åŠ ä¿å­˜é—´éš”
TOTAL_SAVE_CKPT_LIMIT=2  # ä¿ç•™æ›´å¤šæ£€æŸ¥ç‚¹

# LoRAé…ç½® - åŸºäºä¹‹å‰æˆåŠŸç»éªŒ
LORA_R=16
LORA_ALPHA=32
LORA_DROPOUT=0.1

# inference args
INFER_CUDA_IDX="0"
############################################################################
############################ Editable Part Ends ############################
############################################################################
SCRIPT_DIR=$(cd "$(dirname "$0")" && pwd)

# è®¾ç½®æ•°æ®è·¯å¾„
PRETRAIN_DATASET=$SCRIPT_DIR/../output/processed_data/blip2_enhanced_30k_data.jsonl
PRETRAIN_DATASET_IMAGE_PATH=$SCRIPT_DIR/../input/pretrain_stage_1

# ä½¿ç”¨åŸå§‹çš„stage_1.jsonä½œä¸ºå‚è€ƒ
ORIGINAL_DATASET_ALL=$SCRIPT_DIR/../input/pretrain_stage_1/stage_1.json

echo "ğŸš€ å¼€å§‹BLIP2å¢å¼ºæ•°æ®MGM-2B LoRAè®­ç»ƒ"
echo "ğŸ“Š æ•°æ®ç»Ÿè®¡:"
echo "  - è®­ç»ƒæ•°æ®: 17,509æ¡BLIP2å¢å¼ºé«˜è´¨é‡æ•°æ®"
echo "  - å¹³å‡è¯æ•°: 10.67è¯ (vs åŸå§‹8.78è¯)"
echo "  - è¯æ±‡å¤šæ ·æ€§: 0.37 (vs åŸå§‹0.0714)"
echo "  - æ•°æ®æ¥æº: $PRETRAIN_DATASET"
echo ""

# check the global size
PRETRAIN_PASS=`python $SCRIPT_DIR/training/preprocess/check_global_batch_size.py $PRETRAIN_BATCH_SIZE_PER_GPU $PRETRAIN_GRADIENT_ACCUMULATION_STEPS 256`
if [ "$PRETRAIN_PASS" = "False" ]; then
    echo "[ERROR] The global batch size of pretraining stage is not 256! Please check and retry."
    exit
fi
FINETUNE_PASS=`python $SCRIPT_DIR/training/preprocess/check_global_batch_size.py $FINETUNE_BATCH_SIZE_PER_GPU $FINETUNE_GRADIENT_ACCUMULATION_STEPS 128`
if [ "$FINETUNE_PASS" = "False" ]; then
    echo "[ERROR] The global batch size of finetuning stage is not 128! Please check and retry."
    exit
fi

echo "âœ… æ‰¹æ¬¡å¤§å°éªŒè¯é€šè¿‡"
echo "  - é¢„è®­ç»ƒå…¨å±€æ‰¹æ¬¡å¤§å°: 256"
echo "  - å¾®è°ƒå…¨å±€æ‰¹æ¬¡å¤§å°: 128"
echo ""

# æ£€æŸ¥BLIP2æ•°æ®æ ·æœ¬æ•°é‡
BLIP2_SAMPLE_NUM=`wc -l < $PRETRAIN_DATASET`
echo "ğŸ“‹ BLIP2å¢å¼ºæ•°æ®æ ·æœ¬æ•°: $BLIP2_SAMPLE_NUM"

# é™åˆ¶æœ€å¤§æ ·æœ¬æ•°ä»¥æ§åˆ¶è®­ç»ƒæ—¶é—´
MAX_SAMPLE_NUM=20000
if [ $BLIP2_SAMPLE_NUM -gt $MAX_SAMPLE_NUM ]; then
    SAMPLED_PRETRAIN_DATASET=$PRETRAIN_DATASET-${MAX_SAMPLE_NUM}.jsonl
    echo "âš ï¸  æ ·æœ¬æ•°è¶…è¿‡${MAX_SAMPLE_NUM}ï¼Œè¿›è¡Œé‡‡æ ·..."
    python $SCRIPT_DIR/training/preprocess/check_sample_number.py $PRETRAIN_DATASET $SAMPLED_PRETRAIN_DATASET $MAX_SAMPLE_NUM
else
    SAMPLED_PRETRAIN_DATASET=$PRETRAIN_DATASET
    echo "âœ… æ ·æœ¬æ•°åœ¨åˆç†èŒƒå›´å†…ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®"
fi

# convert dataset from dj format to llava format
PRETRAIN_DATASET_JSON=$SAMPLED_PRETRAIN_DATASET.json
echo "ğŸ”„ è½¬æ¢æ•°æ®æ ¼å¼: DJ â†’ LLaVA"
python $SCRIPT_DIR/data-juicer/tools/multimodal/data_juicer_format_to_target_format/dj_to_llava.py $SAMPLED_PRETRAIN_DATASET $PRETRAIN_DATASET_JSON --image_special_token "<__dj__image>" --restore_questions True --original_llava_ds_path $ORIGINAL_DATASET_ALL

# train model
PRETRAIN_NAME=MGM-2B-BLIP2-Pretrain-$EXP_NAME
FINETUNE_NAME=MGM-2B-BLIP2-Finetune-$EXP_NAME
AUX_SIZE=768

NUM_TRAIN_EPOCHS=1
ACTUAL_SAMPLE_NUM=`wc -l < $SAMPLED_PRETRAIN_DATASET`

echo ""
echo "ğŸ¯ è®­ç»ƒé…ç½®:"
echo "  - å®éªŒåç§°: $EXP_NAME"
echo "  - é¢„è®­ç»ƒæ ·æœ¬æ•°: $ACTUAL_SAMPLE_NUM"
echo "  - è®­ç»ƒè½®æ•°: $NUM_TRAIN_EPOCHS"
echo "  - LoRAå‚æ•°: r=$LORA_R, alpha=$LORA_ALPHA, dropout=$LORA_DROPOUT"
echo "  - è¾…åŠ©å›¾åƒå°ºå¯¸: $AUX_SIZE"
echo ""

mkdir -p $SCRIPT_DIR/../output/training_dirs/$PRETRAIN_NAME

echo "ğŸš€ å¼€å§‹é¢„è®­ç»ƒé˜¶æ®µ (LoRA)..."
echo "â° å¼€å§‹æ—¶é—´: $(date)"
PYTHONPATH=$SCRIPT_DIR/training:$PYTHONPATH DS_BUILD_OPS=0 DS_SKIP_CUDA_CHECK=1 /home/robot/lhp/miniconda3/envs/Syn0625/bin/python $(which deepspeed) $SCRIPT_DIR/training/mgm/train/train_mem.py \
    --deepspeed $SCRIPT_DIR/training/scripts/zero3.json \
    --model_name_or_path $SCRIPT_DIR/training/model_zoo/LLM/gemma/gemma-2b-it \
    --version gemma \
    --data_path $PRETRAIN_DATASET_JSON \
    --image_folder $PRETRAIN_DATASET_IMAGE_PATH \
    --vision_tower $SCRIPT_DIR/training/model_zoo/OpenAI/clip-vit-large-patch14-336 \
    --vision_tower_aux $SCRIPT_DIR/training/model_zoo/OpenAI/openclip-convnext-large-d-320-laion2B-s29B-b131K-ft-soup \
    --mm_projector_type mlp2x_gelu \
    --tune_mm_mlp_adapter True \
    --mm_vision_select_layer -2 \
    --mm_use_im_start_end False \
    --mm_use_im_patch_token False \
    --image_size_aux $AUX_SIZE \
    --bf16 True \
    --output_dir $SCRIPT_DIR/../output/training_dirs/$PRETRAIN_NAME \
    --num_train_epochs $NUM_TRAIN_EPOCHS \
    --per_device_train_batch_size $PRETRAIN_BATCH_SIZE_PER_GPU \
    --per_device_eval_batch_size 1 \
    --gradient_accumulation_steps $PRETRAIN_GRADIENT_ACCUMULATION_STEPS \
    --evaluation_strategy "no" \
    --save_strategy "steps" \
    --save_steps $CKPT_SAVE_STEPS \
    --save_total_limit $TOTAL_SAVE_CKPT_LIMIT \
    --learning_rate 1e-3 \
    --weight_decay 0. \
    --warmup_ratio 0.03 \
    --lr_scheduler_type "cosine" \
    --logging_steps $LOGGING_STEP \
    --tf32 True \
    --model_max_length 2048 \
    --gradient_checkpointing True \
    --dataloader_num_workers $PRETRAIN_DATALOADER_NUM_WORKERS \
    --lazy_preprocess True \
    --report_to none \
    --lora_enable True \
    --lora_r $LORA_R \
    --lora_alpha $LORA_ALPHA \
    --lora_dropout $LORA_DROPOUT \
    --lora_bias none \
    2>&1 | tee $SCRIPT_DIR/../output/training_dirs/$PRETRAIN_NAME/pretrain.log

echo "âœ… é¢„è®­ç»ƒå®Œæˆ: $(date)"

mkdir -p $SCRIPT_DIR/../output/training_dirs/$FINETUNE_NAME

echo ""
echo "ğŸš€ å¼€å§‹å¾®è°ƒé˜¶æ®µ (LoRA)..."
echo "â° å¼€å§‹æ—¶é—´: $(date)"
PYTHONPATH=$SCRIPT_DIR/training:$PYTHONPATH DS_BUILD_OPS=0 DS_SKIP_CUDA_CHECK=1 /home/robot/lhp/miniconda3/envs/Syn0625/bin/python $(which deepspeed) $SCRIPT_DIR/training/mgm/train/train_mem.py \
    --deepspeed $SCRIPT_DIR/training/scripts/zero3.json \
    --model_name_or_path $SCRIPT_DIR/training/model_zoo/LLM/gemma/gemma-2b-it \
    --version gemma \
    --data_path $SCRIPT_DIR/training/data/finetuning_stage_1_12k/mgm_instruction_stage_1_12k.json \
    --image_folder $SCRIPT_DIR/training/data/finetuning_stage_1_12k \
    --vision_tower $SCRIPT_DIR/training/model_zoo/OpenAI/clip-vit-large-patch14-336 \
    --vision_tower_aux $SCRIPT_DIR/training/model_zoo/OpenAI/openclip-convnext-large-d-320-laion2B-s29B-b131K-ft-soup \
    --pretrain_mm_mlp_adapter $SCRIPT_DIR/../output/training_dirs/$PRETRAIN_NAME/mm_projector.bin \
    --mm_projector_type mlp2x_gelu \
    --mm_vision_select_layer -2 \
    --mm_use_im_start_end False \
    --mm_use_im_patch_token False \
    --image_aspect_ratio pad \
    --group_by_modality_length True \
    --image_size_aux $AUX_SIZE \
    --bf16 True \
    --output_dir $SCRIPT_DIR/../output/training_dirs/$FINETUNE_NAME \
    --num_train_epochs $NUM_TRAIN_EPOCHS \
    --per_device_train_batch_size $FINETUNE_BATCH_SIZE_PER_GPU \
    --per_device_eval_batch_size 1 \
    --gradient_accumulation_steps $FINETUNE_GRADIENT_ACCUMULATION_STEPS \
    --evaluation_strategy "no" \
    --save_strategy "steps" \
    --save_steps $CKPT_SAVE_STEPS \
    --save_total_limit $TOTAL_SAVE_CKPT_LIMIT \
    --learning_rate 2e-5 \
    --weight_decay 0. \
    --warmup_ratio 0.03 \
    --lr_scheduler_type "cosine" \
    --logging_steps $LOGGING_STEP \
    --tf32 True \
    --model_max_length 1024 \
    --gradient_checkpointing True \
    --dataloader_num_workers $FINETUNE_DATALOADER_NUM_WORKERS \
    --lazy_preprocess True \
    --report_to none \
    --lora_enable True \
    --lora_r $LORA_R \
    --lora_alpha $LORA_ALPHA \
    --lora_dropout $LORA_DROPOUT \
    --lora_bias none \
    2>&1 | tee $SCRIPT_DIR/../output/training_dirs/$FINETUNE_NAME/finetuning.log

echo "âœ… å¾®è°ƒå®Œæˆ: $(date)"

echo ""
echo "ğŸ” å¼€å§‹è¯„ä¼°..."

# inference for submission
# TextVQA
echo "ğŸ“Š TextVQAè¯„ä¼°..."
bash $SCRIPT_DIR/eval/textvqa.sh $FINETUNE_NAME $INFER_CUDA_IDX

# MMBench
echo "ğŸ“Š MMBenchè¯„ä¼°..."
bash $SCRIPT_DIR/eval/mmbench.sh $FINETUNE_NAME "mmbench_dev_20230712" $INFER_CUDA_IDX

# copy this script to output
cp $0 $SCRIPT_DIR/../output/train_blip2_enhanced_lora.sh

echo ""
echo "ğŸ‰ è®­ç»ƒå’Œè¯„ä¼°å®Œæˆ!"
echo "ğŸ“ è®­ç»ƒæ£€æŸ¥ç‚¹: output/training_dirs/$FINETUNE_NAME"
echo "ğŸ“Š è¯„ä¼°ç»“æœ: output/eval_results/$FINETUNE_NAME"
echo "ğŸ“‹ è®­ç»ƒæ—¥å¿—: output/training_dirs/$PRETRAIN_NAME/pretrain.log"
echo "ğŸ“‹ å¾®è°ƒæ—¥å¿—: output/training_dirs/$FINETUNE_NAME/finetuning.log"
echo ""
echo "ğŸ”¬ æ•°æ®è´¨é‡æå‡æ€»ç»“:"
echo "  - ä½¿ç”¨17,509æ¡BLIP2å¢å¼ºæ•°æ®"
echo "  - è¯æ•°æå‡: 8.78 â†’ 10.67è¯ (+21.5%)"
echo "  - è¯æ±‡å¤šæ ·æ€§: 0.0714 â†’ 0.37 (+418%)"
echo "  - LoRAå‚æ•°é«˜æ•ˆå¾®è°ƒï¼Œ24GB VRAMå‹å¥½"
