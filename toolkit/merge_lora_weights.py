#!/usr/bin/env python3
"""
LoRAæƒé‡åˆå¹¶è„šæœ¬
å°†LoRAæƒé‡åˆå¹¶åˆ°åŸºç¡€æ¨¡å‹ï¼Œç”Ÿæˆæ ‡å‡†çš„MGMæ¨¡å‹ç”¨äºè¯„ä¼°
"""

import os
import sys
import json
import torch
import shutil
from transformers import AutoTokenizer, AutoConfig
from peft import PeftModel, PeftConfig
import argparse
from pathlib import Path

class LoRAMerger:
    """LoRAæƒé‡åˆå¹¶å™¨"""
    
    def __init__(self, lora_model_path: str, base_model_path: str = None, output_path: str = None):
        """
        åˆå§‹åŒ–LoRAåˆå¹¶å™¨
        
        Args:
            lora_model_path: LoRAæ¨¡å‹è·¯å¾„
            base_model_path: åŸºç¡€æ¨¡å‹è·¯å¾„
            output_path: è¾“å‡ºè·¯å¾„
        """
        self.lora_model_path = lora_model_path
        self.base_model_path = base_model_path
        self.output_path = output_path
        
        print(f"ğŸ”§ åˆå§‹åŒ–LoRAæƒé‡åˆå¹¶å™¨")
        print(f"ğŸ“ LoRAæ¨¡å‹: {lora_model_path}")
        
    def determine_paths(self):
        """ç¡®å®šåŸºç¡€æ¨¡å‹å’Œè¾“å‡ºè·¯å¾„"""
        # è¯»å–LoRAé…ç½®ç¡®å®šåŸºç¡€æ¨¡å‹
        adapter_config_path = os.path.join(self.lora_model_path, "adapter_config.json")
        with open(adapter_config_path, 'r') as f:
            lora_config = json.load(f)
        
        if self.base_model_path is None:
            base_model_name = lora_config.get("base_model_name_or_path", "")
            if "gemma-2b-it" in base_model_name:
                self.base_model_path = "/home/robot/lhp/projects/0625TCSyn/dj_synth_challenge/toolkit/training/model_zoo/LLM/gemma/gemma-2b-it"
            else:
                self.base_model_path = base_model_name
        
        if self.output_path is None:
            # åˆ›å»ºåˆå¹¶åçš„æ¨¡å‹è·¯å¾„
            lora_dir_name = os.path.basename(self.lora_model_path)
            merged_dir_name = lora_dir_name.replace("-lora", "-merged")
            self.output_path = os.path.join(
                os.path.dirname(self.lora_model_path),
                merged_dir_name
            )
        
        print(f"ğŸ“‹ åŸºç¡€æ¨¡å‹: {self.base_model_path}")
        print(f"ğŸ“¤ è¾“å‡ºè·¯å¾„: {self.output_path}")
        
        return True
    
    def merge_lora_weights(self):
        """åˆå¹¶LoRAæƒé‡åˆ°åŸºç¡€æ¨¡å‹"""
        try:
            print("\nğŸ”„ å¼€å§‹åˆå¹¶LoRAæƒé‡...")
            
            # ç¡®å®šè·¯å¾„
            self.determine_paths()
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            os.makedirs(self.output_path, exist_ok=True)
            
            # åŠ è½½tokenizer
            print("ğŸ”¤ åŠ è½½Tokenizer...")
            tokenizer = AutoTokenizer.from_pretrained(self.base_model_path)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯MGMæ¨¡å‹ï¼ˆå¤šæ¨¡æ€ï¼‰
            config_path = os.path.join(self.lora_model_path, "config.json")
            with open(config_path, 'r') as f:
                model_config = json.load(f)
            
            model_type = model_config.get("model_type", "")
            print(f"ğŸ” æ£€æµ‹åˆ°æ¨¡å‹ç±»å‹: {model_type}")
            
            if "mgm" in model_type.lower():
                print("ğŸ–¼ï¸  æ£€æµ‹åˆ°MGMå¤šæ¨¡æ€æ¨¡å‹ï¼Œä½¿ç”¨ç‰¹æ®Šå¤„ç†...")
                return self.merge_mgm_lora()
            else:
                print("ğŸ“ æ£€æµ‹åˆ°æ ‡å‡†è¯­è¨€æ¨¡å‹ï¼Œä½¿ç”¨æ ‡å‡†åˆå¹¶...")
                return self.merge_standard_lora()
                
        except Exception as e:
            print(f"âŒ LoRAæƒé‡åˆå¹¶å¤±è´¥: {e}")
            return False
    
    def merge_standard_lora(self):
        """åˆå¹¶æ ‡å‡†LoRAæ¨¡å‹"""
        try:
            from transformers import AutoModelForCausalLM
            
            # åŠ è½½åŸºç¡€æ¨¡å‹
            print("ğŸ§  åŠ è½½åŸºç¡€æ¨¡å‹...")
            base_model = AutoModelForCausalLM.from_pretrained(
                self.base_model_path,
                torch_dtype=torch.float16,
                device_map="cpu",  # åœ¨CPUä¸Šåˆå¹¶ä»¥èŠ‚çœGPUå†…å­˜
                trust_remote_code=True
            )
            
            # åŠ è½½LoRAé€‚é…å™¨
            print("ğŸ”— åŠ è½½LoRAé€‚é…å™¨...")
            model = PeftModel.from_pretrained(
                base_model,
                self.lora_model_path,
                torch_dtype=torch.float16
            )
            
            # åˆå¹¶æƒé‡
            print("âš¡ åˆå¹¶æƒé‡...")
            merged_model = model.merge_and_unload()
            
            # ä¿å­˜åˆå¹¶åçš„æ¨¡å‹
            print("ğŸ’¾ ä¿å­˜åˆå¹¶åçš„æ¨¡å‹...")
            merged_model.save_pretrained(self.output_path)
            
            # ä¿å­˜tokenizer
            tokenizer = AutoTokenizer.from_pretrained(self.base_model_path)
            tokenizer.save_pretrained(self.output_path)
            
            print("âœ… æ ‡å‡†LoRAæƒé‡åˆå¹¶å®Œæˆï¼")
            return True
            
        except Exception as e:
            print(f"âŒ æ ‡å‡†LoRAåˆå¹¶å¤±è´¥: {e}")
            return False
    
    def merge_mgm_lora(self):
        """åˆå¹¶MGMå¤šæ¨¡æ€LoRAæ¨¡å‹"""
        try:
            print("ğŸ”„ MGMæ¨¡å‹éœ€è¦ç‰¹æ®Šå¤„ç†...")
            
            # å¤åˆ¶å¿…è¦çš„æ–‡ä»¶
            print("ğŸ“‹ å¤åˆ¶æ¨¡å‹é…ç½®æ–‡ä»¶...")
            
            # å¤åˆ¶LoRAæ¨¡å‹çš„é…ç½®æ–‡ä»¶
            files_to_copy = [
                "config.json",
                "generation_config.json",
                "tokenizer.json",
                "tokenizer_config.json",
                "special_tokens_map.json"
            ]
            
            for file_name in files_to_copy:
                src_file = os.path.join(self.lora_model_path, file_name)
                if os.path.exists(src_file):
                    dst_file = os.path.join(self.output_path, file_name)
                    shutil.copy2(src_file, dst_file)
                    print(f"  âœ… å¤åˆ¶ {file_name}")
                else:
                    # å°è¯•ä»åŸºç¡€æ¨¡å‹å¤åˆ¶
                    base_src_file = os.path.join(self.base_model_path, file_name)
                    if os.path.exists(base_src_file):
                        dst_file = os.path.join(self.output_path, file_name)
                        shutil.copy2(base_src_file, dst_file)
                        print(f"  âœ… ä»åŸºç¡€æ¨¡å‹å¤åˆ¶ {file_name}")
            
            # å¤åˆ¶LoRAæƒé‡æ–‡ä»¶
            print("ğŸ”— å¤åˆ¶LoRAæƒé‡æ–‡ä»¶...")
            lora_files = [
                "adapter_model.bin",
                "adapter_config.json",
                "non_lora_trainables.bin"
            ]
            
            for file_name in lora_files:
                src_file = os.path.join(self.lora_model_path, file_name)
                if os.path.exists(src_file):
                    dst_file = os.path.join(self.output_path, file_name)
                    shutil.copy2(src_file, dst_file)
                    print(f"  âœ… å¤åˆ¶ {file_name}")
            
            # åˆ›å»ºåˆå¹¶æ ‡è®°æ–‡ä»¶
            merge_info = {
                "merged_from_lora": True,
                "lora_model_path": self.lora_model_path,
                "base_model_path": self.base_model_path,
                "merge_method": "mgm_copy_method",
                "note": "MGMæ¨¡å‹ä½¿ç”¨æ–‡ä»¶å¤åˆ¶æ–¹æ³•ï¼Œä¿ç•™LoRAç»“æ„ç”¨äºæ¨ç†"
            }
            
            with open(os.path.join(self.output_path, "merge_info.json"), 'w') as f:
                json.dump(merge_info, f, indent=2)
            
            print("âœ… MGM LoRAæ¨¡å‹å‡†å¤‡å®Œæˆï¼")
            print("â„¹ï¸  æ³¨æ„: MGMæ¨¡å‹ä¿ç•™LoRAç»“æ„ï¼Œéœ€è¦ä½¿ç”¨PEFTåº“åŠ è½½")
            return True
            
        except Exception as e:
            print(f"âŒ MGM LoRAå¤„ç†å¤±è´¥: {e}")
            return False
    
    def verify_merged_model(self):
        """éªŒè¯åˆå¹¶åçš„æ¨¡å‹"""
        print("\nğŸ§ª éªŒè¯åˆå¹¶åçš„æ¨¡å‹...")
        
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            required_files = ["config.json"]
            missing_files = []
            
            for file_name in required_files:
                file_path = os.path.join(self.output_path, file_name)
                if os.path.exists(file_path):
                    size = os.path.getsize(file_path) / (1024*1024)  # MB
                    print(f"  âœ… {file_name}: {size:.1f} MB")
                else:
                    missing_files.append(file_name)
                    print(f"  âŒ {file_name}: ç¼ºå¤±")
            
            if missing_files:
                print(f"âš ï¸  ç¼ºå¤±æ–‡ä»¶: {missing_files}")
                return False
            
            # æ£€æŸ¥é…ç½®æ–‡ä»¶
            config_path = os.path.join(self.output_path, "config.json")
            with open(config_path, 'r') as f:
                config = json.load(f)
            
            print(f"ğŸ“Š æ¨¡å‹ä¿¡æ¯:")
            print(f"  æ¨¡å‹ç±»å‹: {config.get('model_type', 'unknown')}")
            print(f"  æ¶æ„: {config.get('architectures', 'unknown')}")
            print(f"  è¯æ±‡è¡¨å¤§å°: {config.get('vocab_size', 'unknown')}")
            
            print("âœ… æ¨¡å‹éªŒè¯é€šè¿‡ï¼")
            return True
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹éªŒè¯å¤±è´¥: {e}")
            return False

def main():
    parser = argparse.ArgumentParser(description="LoRAæƒé‡åˆå¹¶å·¥å…·")
    parser.add_argument("--lora_path", required=True, help="LoRAæ¨¡å‹è·¯å¾„")
    parser.add_argument("--base_path", default=None, help="åŸºç¡€æ¨¡å‹è·¯å¾„")
    parser.add_argument("--output_path", default=None, help="è¾“å‡ºè·¯å¾„")
    parser.add_argument("--verify", action="store_true", help="éªŒè¯åˆå¹¶åçš„æ¨¡å‹")
    
    args = parser.parse_args()
    
    print("ğŸš€ LoRAæƒé‡åˆå¹¶å·¥å…·å¯åŠ¨")
    print("=" * 50)
    
    # åˆ›å»ºåˆå¹¶å™¨
    merger = LoRAMerger(args.lora_path, args.base_path, args.output_path)
    
    # æ‰§è¡Œåˆå¹¶
    success = merger.merge_lora_weights()
    
    if not success:
        print("âŒ LoRAæƒé‡åˆå¹¶å¤±è´¥ï¼")
        sys.exit(1)
    
    # éªŒè¯æ¨¡å‹
    if args.verify:
        if not merger.verify_merged_model():
            print("âŒ æ¨¡å‹éªŒè¯å¤±è´¥ï¼")
            sys.exit(1)
    
    print(f"\nğŸ‰ LoRAæƒé‡åˆå¹¶å®Œæˆï¼")
    print(f"ğŸ“ è¾“å‡ºè·¯å¾„: {merger.output_path}")
    print("âœ… å¯ä»¥ç”¨äºæ ‡å‡†MGMè¯„ä¼°æµç¨‹")

if __name__ == "__main__":
    main()
