#!/usr/bin/env python3
"""
MMBenchè¯„ä¼°è¿è¡Œè„šæœ¬
æ”¯æŒLoRAæ¨¡åž‹å’Œæ ‡å‡†æ¨¡åž‹çš„MMBenchè¯„ä¼°ï¼Œå¹¶ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
"""

import os
import sys
import subprocess
import json
import argparse
from datetime import datetime

def run_command(cmd, cwd=None):
    """æ‰§è¡Œå‘½ä»¤å¹¶è¿”å›žç»“æžœ"""
    print(f"ðŸ”„ æ‰§è¡Œå‘½ä»¤: {cmd}")
    try:
        result = subprocess.run(cmd, shell=True, cwd=cwd, capture_output=True, text=True)
        if result.returncode != 0:
            print(f"âŒ å‘½ä»¤æ‰§è¡Œå¤±è´¥: {result.stderr}")
            return False, result.stderr
        else:
            print(f"âœ… å‘½ä»¤æ‰§è¡ŒæˆåŠŸ")
            return True, result.stdout
    except Exception as e:
        print(f"âŒ å‘½ä»¤æ‰§è¡Œå¼‚å¸¸: {e}")
        return False, str(e)

def check_model_exists(model_path):
    """æ£€æŸ¥æ¨¡åž‹æ˜¯å¦å­˜åœ¨"""
    if os.path.exists(model_path):
        print(f"âœ… æ¨¡åž‹å­˜åœ¨: {model_path}")
        return True
    else:
        print(f"âŒ æ¨¡åž‹ä¸å­˜åœ¨: {model_path}")
        return False

def check_dataset_exists(dataset_path):
    """æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å­˜åœ¨"""
    if os.path.exists(dataset_path):
        print(f"âœ… æ•°æ®é›†å­˜åœ¨: {dataset_path}")
        return True
    else:
        print(f"âŒ æ•°æ®é›†ä¸å­˜åœ¨: {dataset_path}")
        return False

def run_mmbench_evaluation(model_name, split="mmbench_dev_20230712", gpu_id="0", is_lora=True):
    """è¿è¡ŒMMBenchè¯„ä¼°"""
    
    script_dir = os.path.dirname(os.path.abspath(__file__))
    model_path = os.path.join(script_dir, "../output/training_dirs", model_name)
    dataset_path = os.path.join(script_dir, "training/data/eval_stage_1/mmbench", f"{split}.tsv")
    
    # æ£€æŸ¥æ¨¡åž‹å’Œæ•°æ®é›†
    if not check_model_exists(model_path):
        return False, f"æ¨¡åž‹ä¸å­˜åœ¨: {model_path}"
    
    if not check_dataset_exists(dataset_path):
        return False, f"æ•°æ®é›†ä¸å­˜åœ¨: {dataset_path}"
    
    # é€‰æ‹©è¯„ä¼°è„šæœ¬
    if is_lora:
        eval_script = os.path.join(script_dir, "eval/mmbench_lora.sh")
        print(f"ðŸŽ¯ ä½¿ç”¨LoRAè¯„ä¼°è„šæœ¬: {eval_script}")
    else:
        eval_script = os.path.join(script_dir, "eval/mmbench.sh")
        print(f"ðŸŽ¯ ä½¿ç”¨æ ‡å‡†è¯„ä¼°è„šæœ¬: {eval_script}")
    
    if not os.path.exists(eval_script):
        return False, f"è¯„ä¼°è„šæœ¬ä¸å­˜åœ¨: {eval_script}"
    
    # æž„å»ºè¯„ä¼°å‘½ä»¤
    cmd = f"bash {eval_script} {model_name} {split} {gpu_id}"
    
    print(f"ðŸš€ å¼€å§‹MMBenchè¯„ä¼°")
    print(f"ðŸ“ æ¨¡åž‹: {model_name}")
    print(f"ðŸ“Š æ•°æ®é›†: {split}")
    print(f"ðŸ–¥ï¸  GPU: {gpu_id}")
    print(f"ðŸ”§ LoRAæ¨¡å¼: {is_lora}")
    
    # æ‰§è¡Œè¯„ä¼°
    success, output = run_command(cmd, cwd=script_dir)
    
    if success:
        # æ£€æŸ¥ç»“æžœæ–‡ä»¶
        result_file = os.path.join(script_dir, "../output/eval_results", model_name, "mmbench/answers", split, f"{model_name}.jsonl")
        if os.path.exists(result_file):
            with open(result_file, 'r') as f:
                result_count = sum(1 for _ in f)
            print(f"âœ… MMBenchè¯„ä¼°å®Œæˆ: {result_count} ä¸ªç»“æžœ")
            return True, result_file
        else:
            print(f"âš ï¸  è¯„ä¼°å®Œæˆä½†ç»“æžœæ–‡ä»¶ä¸å­˜åœ¨: {result_file}")
            return False, "ç»“æžœæ–‡ä»¶ä¸å­˜åœ¨"
    else:
        return False, output

def analyze_mmbench_results(result_file):
    """åˆ†æžMMBenchè¯„ä¼°ç»“æžœ"""
    if not os.path.exists(result_file):
        return None
    
    print(f"ðŸ“Š åˆ†æžMMBenchç»“æžœ: {result_file}")
    
    results = []
    with open(result_file, 'r') as f:
        for line in f:
            if line.strip():
                results.append(json.loads(line))
    
    total_questions = len(results)
    
    # ç®€å•ç»Ÿè®¡ï¼ˆè¿™é‡Œå¯ä»¥æ ¹æ®éœ€è¦æ‰©å±•æ›´è¯¦ç»†çš„åˆ†æžï¼‰
    analysis = {
        "total_questions": total_questions,
        "model_responses": len([r for r in results if r.get('text', '').strip()]),
        "avg_response_length": sum(len(r.get('text', '')) for r in results) / total_questions if total_questions > 0 else 0,
        "sample_responses": results[:3] if len(results) >= 3 else results
    }
    
    print(f"ðŸ“ˆ åˆ†æžç»“æžœ:")
    print(f"  - æ€»é—®é¢˜æ•°: {analysis['total_questions']}")
    print(f"  - æœ‰æ•ˆå“åº”: {analysis['model_responses']}")
    print(f"  - å¹³å‡å“åº”é•¿åº¦: {analysis['avg_response_length']:.1f} å­—ç¬¦")
    
    return analysis

def main():
    parser = argparse.ArgumentParser(description="MMBenchè¯„ä¼°è¿è¡Œè„šæœ¬")
    parser.add_argument("--models", nargs="+", required=True, help="è¦è¯„ä¼°çš„æ¨¡åž‹åç§°åˆ—è¡¨")
    parser.add_argument("--split", default="mmbench_dev_20230712", help="æ•°æ®é›†åˆ†å‰²")
    parser.add_argument("--gpu", default="0", help="GPU ID")
    parser.add_argument("--lora", action="store_true", help="ä½¿ç”¨LoRAè¯„ä¼°æ¨¡å¼")
    parser.add_argument("--output-report", help="è¾“å‡ºå¯¹æ¯”æŠ¥å‘Šè·¯å¾„")
    
    args = parser.parse_args()
    
    print("ðŸŽ¯ MMBenchè¯„ä¼°ä»»åŠ¡å¼€å§‹")
    print(f"ðŸ“‹ æ¨¡åž‹åˆ—è¡¨: {args.models}")
    print(f"ðŸ“Š æ•°æ®é›†: {args.split}")
    print(f"ðŸ–¥ï¸  GPU: {args.gpu}")
    print(f"ðŸ”§ LoRAæ¨¡å¼: {args.lora}")
    
    evaluation_results = {}
    
    # é€ä¸ªè¯„ä¼°æ¨¡åž‹
    for model_name in args.models:
        print(f"\n{'='*60}")
        print(f"ðŸ”„ è¯„ä¼°æ¨¡åž‹: {model_name}")
        print(f"{'='*60}")
        
        success, result = run_mmbench_evaluation(
            model_name=model_name,
            split=args.split,
            gpu_id=args.gpu,
            is_lora=args.lora
        )
        
        if success:
            # åˆ†æžç»“æžœ
            analysis = analyze_mmbench_results(result)
            evaluation_results[model_name] = {
                "success": True,
                "result_file": result,
                "analysis": analysis,
                "timestamp": datetime.now().isoformat()
            }
        else:
            evaluation_results[model_name] = {
                "success": False,
                "error": result,
                "timestamp": datetime.now().isoformat()
            }
    
    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    print(f"\n{'='*60}")
    print("ðŸ“Š ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š")
    print(f"{'='*60}")
    
    report = {
        "evaluation_config": {
            "models": args.models,
            "split": args.split,
            "gpu": args.gpu,
            "lora_mode": args.lora,
            "timestamp": datetime.now().isoformat()
        },
        "results": evaluation_results
    }
    
    # ä¿å­˜æŠ¥å‘Š
    if args.output_report:
        report_path = args.output_report
    else:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        report_path = os.path.join(script_dir, "../output/eval_results", f"mmbench_evaluation_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
    
    os.makedirs(os.path.dirname(report_path), exist_ok=True)
    
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print(f"ðŸ“„ è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    # æ‰“å°æ€»ç»“
    print(f"\n{'='*60}")
    print("ðŸ“ˆ è¯„ä¼°æ€»ç»“")
    print(f"{'='*60}")
    
    successful_models = [name for name, result in evaluation_results.items() if result["success"]]
    failed_models = [name for name, result in evaluation_results.items() if not result["success"]]
    
    print(f"âœ… æˆåŠŸè¯„ä¼°: {len(successful_models)} ä¸ªæ¨¡åž‹")
    for model in successful_models:
        analysis = evaluation_results[model]["analysis"]
        if analysis:
            print(f"  - {model}: {analysis['total_questions']} é—®é¢˜, {analysis['model_responses']} å“åº”")
    
    if failed_models:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {len(failed_models)} ä¸ªæ¨¡åž‹")
        for model in failed_models:
            print(f"  - {model}: {evaluation_results[model]['error']}")
    
    print(f"\nðŸŽ‰ MMBenchè¯„ä¼°ä»»åŠ¡å®Œæˆï¼")
    return len(successful_models) > 0

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
