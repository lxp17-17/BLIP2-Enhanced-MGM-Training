#!/usr/bin/env python3
"""
LoRAæ¨ç†å¼•æ“
æ”¯æŒLoRAæ¨¡å‹çš„å®Œæ•´æ¨ç†åŠŸèƒ½
"""

import os
import sys
import json
import torch
import numpy as np
from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM
from peft import PeftModel, PeftConfig
from PIL import Image
import argparse
from typing import List, Dict, Any, Optional

class LoRAInferenceEngine:
    """LoRAæ¨¡å‹æ¨ç†å¼•æ“"""
    
    def __init__(self, lora_model_path: str, base_model_path: str = None):
        """
        åˆå§‹åŒ–LoRAæ¨ç†å¼•æ“
        
        Args:
            lora_model_path: LoRAæ¨¡å‹è·¯å¾„
            base_model_path: åŸºç¡€æ¨¡å‹è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä»LoRAé…ç½®ä¸­è·å–
        """
        self.lora_model_path = lora_model_path
        self.base_model_path = base_model_path
        self.model = None
        self.tokenizer = None
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        print(f"ğŸ”§ åˆå§‹åŒ–LoRAæ¨ç†å¼•æ“")
        print(f"ğŸ“ LoRAæ¨¡å‹è·¯å¾„: {lora_model_path}")
        print(f"ğŸ–¥ï¸  è®¾å¤‡: {self.device}")
        
    def load_model(self):
        """åŠ è½½LoRAæ¨¡å‹"""
        try:
            # è¯»å–LoRAé…ç½®
            adapter_config_path = os.path.join(self.lora_model_path, "adapter_config.json")
            with open(adapter_config_path, 'r') as f:
                lora_config = json.load(f)
            
            # ç¡®å®šåŸºç¡€æ¨¡å‹è·¯å¾„
            if self.base_model_path is None:
                base_model_name = lora_config.get("base_model_name_or_path", "")
                if "gemma-2b-it" in base_model_name:
                    self.base_model_path = "/home/robot/lhp/projects/0625TCSyn/dj_synth_challenge/toolkit/training/model_zoo/LLM/gemma/gemma-2b-it"
                else:
                    self.base_model_path = base_model_name
            
            print(f"ğŸ“‹ åŸºç¡€æ¨¡å‹: {self.base_model_path}")
            print(f"ğŸ”§ LoRAé…ç½®: rank={lora_config.get('r')}, alpha={lora_config.get('lora_alpha')}")
            
            # åŠ è½½tokenizer
            print("ğŸ”¤ åŠ è½½Tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(self.base_model_path)
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # åŠ è½½åŸºç¡€æ¨¡å‹
            print("ğŸ§  åŠ è½½åŸºç¡€æ¨¡å‹...")
            base_model = AutoModelForCausalLM.from_pretrained(
                self.base_model_path,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True
            )
            
            # åŠ è½½LoRAé€‚é…å™¨
            print("ğŸ”— åŠ è½½LoRAé€‚é…å™¨...")
            self.model = PeftModel.from_pretrained(
                base_model,
                self.lora_model_path,
                torch_dtype=torch.float16
            )
            
            # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            self.model.eval()
            
            print("âœ… LoRAæ¨¡å‹åŠ è½½æˆåŠŸï¼")
            return True
            
        except Exception as e:
            print(f"âŒ LoRAæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    def generate_text(self, prompt: str, max_length: int = 512, temperature: float = 0.7) -> str:
        """
        ç”Ÿæˆæ–‡æœ¬
        
        Args:
            prompt: è¾“å…¥æç¤º
            max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
            temperature: ç”Ÿæˆæ¸©åº¦
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬
        """
        if self.model is None or self.tokenizer is None:
            raise RuntimeError("æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨load_model()")
        
        try:
            # ç¼–ç è¾“å…¥
            inputs = self.tokenizer.encode(prompt, return_tensors="pt").to(self.device)
            
            # ç”Ÿæˆæ–‡æœ¬
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    max_length=max_length,
                    temperature=temperature,
                    do_sample=True if temperature > 0 else False,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    num_return_sequences=1
                )
            
            # è§£ç è¾“å‡º
            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # ç§»é™¤è¾“å…¥éƒ¨åˆ†
            if generated_text.startswith(prompt):
                generated_text = generated_text[len(prompt):].strip()
            
            return generated_text
            
        except Exception as e:
            print(f"âŒ æ–‡æœ¬ç”Ÿæˆå¤±è´¥: {e}")
            return ""
    
    def test_basic_inference(self) -> bool:
        """æµ‹è¯•åŸºç¡€æ¨ç†åŠŸèƒ½"""
        print("\nğŸ§ª æµ‹è¯•åŸºç¡€æ¨ç†åŠŸèƒ½...")
        
        test_prompts = [
            "Hello, how are you?",
            "What is the capital of France?",
            "Describe a beautiful sunset.",
        ]
        
        success_count = 0
        for i, prompt in enumerate(test_prompts, 1):
            print(f"\næµ‹è¯• {i}: {prompt}")
            try:
                response = self.generate_text(prompt, max_length=100, temperature=0.1)
                if response and len(response.strip()) > 0:
                    print(f"âœ… å“åº”: {response[:100]}...")
                    success_count += 1
                else:
                    print("âŒ ç”Ÿæˆç©ºå“åº”")
            except Exception as e:
                print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
        
        success_rate = success_count / len(test_prompts)
        print(f"\nğŸ“Š åŸºç¡€æ¨ç†æµ‹è¯•ç»“æœ: {success_count}/{len(test_prompts)} ({success_rate*100:.1f}%)")
        
        return success_rate > 0.5
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        if self.model is None:
            return {"status": "not_loaded"}
        
        # è®¡ç®—å‚æ•°æ•°é‡
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        
        return {
            "status": "loaded",
            "device": str(self.device),
            "total_parameters": total_params,
            "trainable_parameters": trainable_params,
            "lora_parameters": trainable_params,
            "base_model_path": self.base_model_path,
            "lora_model_path": self.lora_model_path,
            "vocab_size": len(self.tokenizer) if self.tokenizer else 0
        }

def main():
    parser = argparse.ArgumentParser(description="LoRAæ¨ç†å¼•æ“æµ‹è¯•")
    parser.add_argument("--lora_path", required=True, help="LoRAæ¨¡å‹è·¯å¾„")
    parser.add_argument("--base_path", default=None, help="åŸºç¡€æ¨¡å‹è·¯å¾„")
    parser.add_argument("--test_inference", action="store_true", help="æµ‹è¯•æ¨ç†åŠŸèƒ½")
    
    args = parser.parse_args()
    
    print("ğŸš€ LoRAæ¨ç†å¼•æ“å¯åŠ¨")
    print("=" * 50)
    
    # åˆ›å»ºæ¨ç†å¼•æ“
    engine = LoRAInferenceEngine(args.lora_path, args.base_path)
    
    # åŠ è½½æ¨¡å‹
    if not engine.load_model():
        print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œé€€å‡º")
        sys.exit(1)
    
    # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
    model_info = engine.get_model_info()
    print(f"\nğŸ“Š æ¨¡å‹ä¿¡æ¯:")
    print(f"  æ€»å‚æ•°: {model_info['total_parameters']:,}")
    print(f"  LoRAå‚æ•°: {model_info['lora_parameters']:,}")
    print(f"  è¯æ±‡è¡¨å¤§å°: {model_info['vocab_size']:,}")
    
    # æµ‹è¯•æ¨ç†
    if args.test_inference:
        success = engine.test_basic_inference()
        if success:
            print("\nğŸ‰ LoRAæ¨ç†å¼•æ“æµ‹è¯•æˆåŠŸï¼")
        else:
            print("\nâŒ LoRAæ¨ç†å¼•æ“æµ‹è¯•å¤±è´¥ï¼")
            sys.exit(1)
    
    print("\nâœ… LoRAæ¨ç†å¼•æ“å°±ç»ª")

if __name__ == "__main__":
    main()
