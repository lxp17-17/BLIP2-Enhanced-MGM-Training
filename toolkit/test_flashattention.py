#!/usr/bin/env python3

import subprocess
import sys
import os
import time

def test_attention_implementation(attn_impl, test_name):
    """æµ‹è¯•ä¸åŒçš„attentionå®ç°"""
    
    print(f"\n{'='*60}")
    print(f"æµ‹è¯• {test_name} (attn_implementation={attn_impl})")
    print(f"{'='*60}")
    
    # ä¿®æ”¹train_mem.pyä½¿ç”¨æŒ‡å®šçš„attentionå®ç°
    train_mem_path = "training/mgm/train/train_mem.py"
    
    # è¯»å–åŸæ–‡ä»¶
    with open(train_mem_path, 'r') as f:
        content = f.read()
    
    # æ›¿æ¢attentionå®ç°
    if attn_impl == "flash_attention_2":
        new_content = content.replace(
            'train(attn_implementation="sdpa")',
            'train(attn_implementation="flash_attention_2")'
        )
    else:
        new_content = content.replace(
            'train(attn_implementation="flash_attention_2")',
            'train(attn_implementation="sdpa")'
        )
    
    # å†™å…¥ä¿®æ”¹åçš„æ–‡ä»¶
    with open(train_mem_path, 'w') as f:
        f.write(new_content)
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    env = os.environ.copy()
    env['CUDA_VISIBLE_DEVICES'] = '0'
    env['CUDA_HOME'] = '/home/robot/lhp/miniconda3/envs/Syn0625'
    env['DS_BUILD_OPS'] = '0'
    env['DS_SKIP_CUDA_CHECK'] = '1'
    env['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
    
    # æ„å»ºè®­ç»ƒå‘½ä»¤ - ä½¿ç”¨æœ€å°å‚æ•°å¿«é€Ÿæµ‹è¯•
    cmd = [
        '/home/robot/lhp/miniconda3/envs/Syn0625/bin/python', 
        'training/mgm/train/train_mem.py',
        '--deepspeed', 'training/scripts/zero3.json',
        '--model_name_or_path', 'training/model_zoo/LLM/gemma/gemma-2b-it',
        '--version', 'gemma',
        '--data_path', 'training/data/finetuning_stage_1_12k/mgm_instruction_stage_1_12k.json',
        '--image_folder', 'training/data/finetuning_stage_1_12k',
        '--vision_tower', 'training/model_zoo/OpenAI/clip-vit-large-patch14-336',
        '--vision_tower_aux', 'training/model_zoo/OpenAI/openclip-convnext-large-d-320-laion2B-s29B-b131K-ft-soup',
        '--pretrain_mm_mlp_adapter', '../output/training_dirs/MGM-2B-Pretrain-default/mm_projector.bin',
        '--mm_projector_type', 'mlp2x_gelu',
        '--mm_vision_select_layer', '-2',
        '--mm_use_im_start_end', 'False',
        '--mm_use_im_patch_token', 'False',
        '--image_aspect_ratio', 'pad',
        '--group_by_modality_length', 'True',
        '--image_size_aux', '768',
        '--fp16', 'True',
        '--output_dir', f'../output/training_dirs/MGM-2B-Test-{attn_impl}',
        '--num_train_epochs', '1',
        '--per_device_train_batch_size', '1',
        '--per_device_eval_batch_size', '1',
        '--gradient_accumulation_steps', '128',
        '--evaluation_strategy', 'no',
        '--save_strategy', 'steps',
        '--save_steps', '50',
        '--save_total_limit', '1',
        '--learning_rate', '2e-5',
        '--weight_decay', '0.',
        '--warmup_ratio', '0.03',
        '--lr_scheduler_type', 'cosine',
        '--logging_steps', '1',
        '--tf32', 'True',
        '--model_max_length', '1024',
        '--gradient_checkpointing', 'True',
        '--dataloader_num_workers', '1',
        '--lazy_preprocess', 'True',
        '--report_to', 'none',
        '--max_steps', '5'  # åªè¿è¡Œ5æ­¥è¿›è¡Œæµ‹è¯•
    ]
    
    print(f"è¿è¡Œå‘½ä»¤: {' '.join(cmd[:3])} ...")
    print(f"Attentionå®ç°: {attn_impl}")
    
    start_time = time.time()
    
    try:
        # è¿è¡Œè®­ç»ƒå‘½ä»¤
        result = subprocess.run(
            cmd, 
            env=env,
            capture_output=True, 
            text=True, 
            timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
        )
        
        end_time = time.time()
        duration = end_time - start_time
        
        if result.returncode == 0:
            print(f"âœ… {test_name} æˆåŠŸ!")
            print(f"â±ï¸  è¿è¡Œæ—¶é—´: {duration:.2f}ç§’")
            
            # æ£€æŸ¥è¾“å‡ºä¸­çš„æ˜¾å­˜ä½¿ç”¨ä¿¡æ¯
            if "Parameter Offload" in result.stdout:
                print("ğŸ“Š å‘ç°å‚æ•°offloadä¿¡æ¯")
            
            return True, duration, result.stdout, result.stderr
            
        else:
            print(f"âŒ {test_name} å¤±è´¥!")
            print(f"è¿”å›ç : {result.returncode}")
            print(f"â±ï¸  è¿è¡Œæ—¶é—´: {duration:.2f}ç§’")
            
            # æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯çš„æœ€åå‡ è¡Œ
            if result.stderr:
                stderr_lines = result.stderr.strip().split('\n')
                print("é”™è¯¯ä¿¡æ¯ (æœ€å10è¡Œ):")
                for line in stderr_lines[-10:]:
                    print(f"  {line}")
            
            return False, duration, result.stdout, result.stderr
            
    except subprocess.TimeoutExpired:
        print(f"â° {test_name} è¶…æ—¶ (5åˆ†é’Ÿ)")
        return False, 300, "", "Timeout"
    
    except Exception as e:
        print(f"ğŸ’¥ {test_name} å¼‚å¸¸: {e}")
        return False, 0, "", str(e)

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    
    print("ğŸš€ FlashAttention vs SDPA å¯¹æ¯”æµ‹è¯•")
    print("ç›®æ ‡: æµ‹è¯•å“ªç§attentionå®ç°åœ¨MGM-2Bä¸Šæ˜¾å­˜ä½¿ç”¨æ›´ä¼˜")
    
    results = {}
    
    # æµ‹è¯•1: SDPA (å½“å‰ä½¿ç”¨çš„)
    success1, time1, stdout1, stderr1 = test_attention_implementation("sdpa", "SDPA (å½“å‰)")
    results["SDPA"] = {
        "success": success1,
        "time": time1,
        "stdout": stdout1,
        "stderr": stderr1
    }
    
    # æµ‹è¯•2: FlashAttention 2
    success2, time2, stdout2, stderr2 = test_attention_implementation("flash_attention_2", "FlashAttention 2")
    results["FlashAttention"] = {
        "success": success2,
        "time": time2,
        "stdout": stdout2,
        "stderr": stderr2
    }
    
    # æ€»ç»“ç»“æœ
    print(f"\n{'='*60}")
    print("ğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“")
    print(f"{'='*60}")
    
    for name, result in results.items():
        status = "âœ… æˆåŠŸ" if result["success"] else "âŒ å¤±è´¥"
        print(f"{name:15} | {status:8} | {result['time']:6.2f}ç§’")
    
    # æ¨è
    if results["FlashAttention"]["success"] and results["SDPA"]["success"]:
        if results["FlashAttention"]["time"] < results["SDPA"]["time"]:
            print(f"\nğŸ¯ æ¨è: FlashAttention (æ›´å¿« {results['SDPA']['time'] - results['FlashAttention']['time']:.2f}ç§’)")
        else:
            print(f"\nğŸ¯ æ¨è: SDPA (æ›´ç¨³å®š)")
    elif results["FlashAttention"]["success"]:
        print(f"\nğŸ¯ æ¨è: FlashAttention (SDPAå¤±è´¥)")
    elif results["SDPA"]["success"]:
        print(f"\nğŸ¯ æ¨è: SDPA (FlashAttentionå¤±è´¥)")
    else:
        print(f"\nâŒ ä¸¤ç§å®ç°éƒ½å¤±è´¥äº†ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
    
    # æ¢å¤åŸå§‹è®¾ç½® (SDPA)
    test_attention_implementation("sdpa", "æ¢å¤åŸå§‹è®¾ç½®")
    print(f"\nğŸ”„ å·²æ¢å¤ä¸ºSDPAè®¾ç½®")

if __name__ == "__main__":
    main()
