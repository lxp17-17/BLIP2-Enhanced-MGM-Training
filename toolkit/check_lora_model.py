#!/usr/bin/env python3
"""
ç®€åŒ–çš„LoRAæ¨¡å‹æ£€æŸ¥è„šæœ¬
æ£€æŸ¥LoRAæ¨¡å‹çš„åŸºæœ¬ä¿¡æ¯å’Œå…¼å®¹æ€§
"""

import os
import json
import torch
import sys

def check_lora_model(model_path):
    """æ£€æŸ¥LoRAæ¨¡å‹çš„åŸºæœ¬ä¿¡æ¯"""
    print(f"ğŸ” æ£€æŸ¥LoRAæ¨¡å‹: {model_path}")
    print("=" * 60)
    
    # æ£€æŸ¥å¿…è¦æ–‡ä»¶
    required_files = [
        "adapter_config.json",
        "adapter_model.bin", 
        "config.json"
    ]
    
    missing_files = []
    for file in required_files:
        file_path = os.path.join(model_path, file)
        if os.path.exists(file_path):
            size = os.path.getsize(file_path) / (1024*1024)  # MB
            print(f"âœ… {file}: {size:.1f} MB")
        else:
            missing_files.append(file)
            print(f"âŒ {file}: ç¼ºå¤±")
    
    if missing_files:
        print(f"\nâš ï¸  ç¼ºå¤±æ–‡ä»¶: {missing_files}")
        return False
    
    print("\nğŸ“‹ LoRAé…ç½®ä¿¡æ¯:")
    # è¯»å–LoRAé…ç½®
    try:
        with open(os.path.join(model_path, "adapter_config.json"), 'r') as f:
            lora_config = json.load(f)
        
        print(f"  åŸºç¡€æ¨¡å‹: {lora_config.get('base_model_name_or_path', 'unknown')}")
        print(f"  LoRA rank: {lora_config.get('r', 'unknown')}")
        print(f"  LoRA alpha: {lora_config.get('lora_alpha', 'unknown')}")
        print(f"  LoRA dropout: {lora_config.get('lora_dropout', 'unknown')}")
        print(f"  ç›®æ ‡æ¨¡å—: {lora_config.get('target_modules', 'unknown')}")
        
    except Exception as e:
        print(f"âŒ è¯»å–LoRAé…ç½®å¤±è´¥: {e}")
        return False
    
    print("\nğŸ”§ æ¨¡å‹é…ç½®ä¿¡æ¯:")
    # è¯»å–æ¨¡å‹é…ç½®
    try:
        with open(os.path.join(model_path, "config.json"), 'r') as f:
            model_config = json.load(f)
        
        print(f"  æ¨¡å‹ç±»å‹: {model_config.get('model_type', 'unknown')}")
        print(f"  æ¶æ„: {model_config.get('architectures', 'unknown')}")
        print(f"  è¯æ±‡è¡¨å¤§å°: {model_config.get('vocab_size', 'unknown')}")
        print(f"  éšè—å±‚å¤§å°: {model_config.get('hidden_size', 'unknown')}")
        
    except Exception as e:
        print(f"âŒ è¯»å–æ¨¡å‹é…ç½®å¤±è´¥: {e}")
        return False
    
    print("\nğŸ“Š æ–‡ä»¶å¤§å°ç»Ÿè®¡:")
    total_size = 0
    for file in os.listdir(model_path):
        if file.endswith(('.bin', '.json', '.txt')):
            file_path = os.path.join(model_path, file)
            size = os.path.getsize(file_path) / (1024*1024)  # MB
            total_size += size
            print(f"  {file}: {size:.1f} MB")
    
    print(f"\nğŸ“¦ æ€»å¤§å°: {total_size:.1f} MB")
    
    # æ£€æŸ¥æ˜¯å¦å¯ä»¥åŠ è½½LoRAæƒé‡
    print("\nğŸ§ª æƒé‡åŠ è½½æµ‹è¯•:")
    try:
        adapter_path = os.path.join(model_path, "adapter_model.bin")
        weights = torch.load(adapter_path, map_location='cpu')
        print(f"âœ… LoRAæƒé‡åŠ è½½æˆåŠŸï¼ŒåŒ…å« {len(weights)} ä¸ªå‚æ•°")
        
        # æ˜¾ç¤ºå‰å‡ ä¸ªæƒé‡çš„ä¿¡æ¯
        for i, (key, tensor) in enumerate(list(weights.items())[:5]):
            print(f"  {key}: {tensor.shape} ({tensor.dtype})")
        
        if len(weights) > 5:
            print(f"  ... è¿˜æœ‰ {len(weights)-5} ä¸ªæƒé‡")
            
    except Exception as e:
        print(f"âŒ LoRAæƒé‡åŠ è½½å¤±è´¥: {e}")
        return False
    
    print("\nâœ… LoRAæ¨¡å‹æ£€æŸ¥å®Œæˆï¼")
    print("ğŸ¯ æ¨¡å‹æ–‡ä»¶å®Œæ•´ï¼Œå¯ä»¥ç”¨äºæ¨ç†æˆ–è¿›ä¸€æ­¥è¯„ä¼°")
    return True

def main():
    if len(sys.argv) != 2:
        print("ç”¨æ³•: python check_lora_model.py <model_path>")
        sys.exit(1)
    
    model_path = sys.argv[1]
    
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        sys.exit(1)
    
    success = check_lora_model(model_path)
    
    if not success:
        print("\nâŒ æ¨¡å‹æ£€æŸ¥å¤±è´¥ï¼")
        sys.exit(1)
    else:
        print("\nğŸ‰ æ¨¡å‹æ£€æŸ¥æˆåŠŸï¼")

if __name__ == "__main__":
    main()
