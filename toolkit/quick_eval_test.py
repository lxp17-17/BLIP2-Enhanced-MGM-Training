#!/usr/bin/env python3

"""
å¿«é€ŸLoRAæ¨¡å‹è¯„ä¼°æµ‹è¯•
æµ‹è¯•å°‘é‡æ ·æœ¬éªŒè¯æ¨¡å‹æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import os
import json
import torch
from PIL import Image
from tqdm import tqdm
import argparse

def quick_eval_test(model_path, num_samples=5):
    """å¿«é€Ÿè¯„ä¼°æµ‹è¯•"""
    
    print("ğŸš€ å¼€å§‹å¿«é€ŸLoRAæ¨¡å‹è¯„ä¼°æµ‹è¯•...")
    print(f"æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"æµ‹è¯•æ ·æœ¬æ•°: {num_samples}")
    
    try:
        # è®¾ç½®ç¯å¢ƒ
        from mgm.model.builder import load_pretrained_model
        from mgm.utils import disable_torch_init
        from mgm.conversation import conv_templates
        from mgm.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN
        from mgm.mm_utils import tokenizer_image_token, get_model_name_from_path
        
        disable_torch_init()
        
        # æ£€æŸ¥LoRAæ¨¡å‹
        adapter_config_path = os.path.join(model_path, "adapter_config.json")
        if not os.path.exists(adapter_config_path):
            print("âŒ æœªæ‰¾åˆ°LoRAé…ç½®æ–‡ä»¶")
            return False
            
        with open(adapter_config_path, 'r') as f:
            adapter_config = json.load(f)
        base_model_path = adapter_config.get("base_model_name_or_path")
        print(f"âœ… LoRAé…ç½®: r={adapter_config.get('r')}, alpha={adapter_config.get('lora_alpha')}")
        print(f"ğŸ“ åŸºç¡€æ¨¡å‹: {base_model_path}")
        
        # åŠ è½½æ¨¡å‹ - ä½¿ç”¨åŸºç¡€æ¨¡å‹è·¯å¾„ä½†åŠ è½½LoRAæƒé‡
        print("ğŸ“¥ åŠ è½½æ¨¡å‹...")
        model_name = get_model_name_from_path(model_path)
        
        # ç›´æ¥ä½¿ç”¨LoRAæ¨¡å‹è·¯å¾„ï¼Œè®©MGMçš„builderå¤„ç†
        tokenizer, model, image_processor, context_len = load_pretrained_model(
            model_path, base_model_path, model_name
        )
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # åŠ è½½æµ‹è¯•æ•°æ®
        test_data_path = "/home/robot/lhp/projects/0625TCSyn/dj_synth_challenge/toolkit/training/data/eval_stage_1/textvqa/llava_textvqa_val_v051_ocr.jsonl"
        image_folder = "/home/robot/lhp/projects/0625TCSyn/dj_synth_challenge/toolkit/training/data/eval_stage_1/textvqa/train_images"
        
        print(f"ğŸ“‚ åŠ è½½æµ‹è¯•æ•°æ®: {test_data_path}")
        
        with open(test_data_path, 'r') as f:
            test_data = [json.loads(line) for line in f]
        
        # åªæµ‹è¯•å‰å‡ ä¸ªæ ·æœ¬
        test_samples = test_data[:num_samples]
        print(f"ğŸ¯ å¼€å§‹æµ‹è¯• {len(test_samples)} ä¸ªæ ·æœ¬...")
        
        results = []
        success_count = 0
        
        for i, sample in enumerate(tqdm(test_samples, desc="è¯„ä¼°è¿›åº¦")):
            try:
                question_id = sample["question_id"]
                image_file = sample["image"]
                question = sample["text"]
                
                # åŠ è½½å›¾åƒ
                image_path = os.path.join(image_folder, image_file)
                if not os.path.exists(image_path):
                    print(f"âš ï¸  å›¾åƒä¸å­˜åœ¨: {image_path}")
                    continue
                
                image = Image.open(image_path).convert('RGB')
                
                # å‡†å¤‡è¾“å…¥
                qs = question
                if model.config.mm_use_im_start_end:
                    qs = DEFAULT_IMAGE_TOKEN + '\n' + qs
                else:
                    qs = DEFAULT_IMAGE_TOKEN + '\n' + qs
                
                conv = conv_templates["gemma"].copy()
                conv.append_message(conv.roles[0], qs)
                conv.append_message(conv.roles[1], None)
                prompt = conv.get_prompt()
                
                # Tokenize
                input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()
                
                # å¤„ç†å›¾åƒ
                image_tensor = image_processor.preprocess(image, return_tensors='pt')['pixel_values'][0]
                
                # æ¨ç†
                with torch.inference_mode():
                    output_ids = model.generate(
                        input_ids,
                        images=image_tensor.unsqueeze(0).half().cuda(),
                        do_sample=False,
                        temperature=0,
                        max_new_tokens=128,
                        use_cache=True
                    )
                
                # è§£ç è¾“å‡º
                input_token_len = input_ids.shape[1]
                outputs = tokenizer.batch_decode(output_ids[:, input_token_len:], skip_special_tokens=True)[0]
                outputs = outputs.strip()
                
                result = {
                    "question_id": question_id,
                    "question": question,
                    "answer": outputs,
                    "image": image_file
                }
                results.append(result)
                success_count += 1
                
                print(f"âœ… æ ·æœ¬ {i+1}/{len(test_samples)}")
                print(f"   é—®é¢˜: {question[:100]}...")
                print(f"   å›ç­”: {outputs}")
                print()
                
            except Exception as e:
                print(f"âŒ æ ·æœ¬ {i+1} å¤±è´¥: {e}")
                continue
        
        # ä¿å­˜ç»“æœ
        output_file = "/home/robot/lhp/projects/0625TCSyn/dj_synth_challenge/output/ç»“æœåˆ†æ/quick_eval_results.json"
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ‰ è¯„ä¼°å®Œæˆ!")
        print(f"âœ… æˆåŠŸæ ·æœ¬: {success_count}/{len(test_samples)}")
        print(f"ğŸ“ ç»“æœä¿å­˜è‡³: {output_file}")
        
        return success_count > 0
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    parser = argparse.ArgumentParser(description='å¿«é€ŸLoRAæ¨¡å‹è¯„ä¼°æµ‹è¯•')
    parser.add_argument('--model-path', required=True, help='LoRAæ¨¡å‹è·¯å¾„')
    parser.add_argument('--num-samples', type=int, default=5, help='æµ‹è¯•æ ·æœ¬æ•°é‡')
    
    args = parser.parse_args()
    
    success = quick_eval_test(args.model_path, args.num_samples)
    
    if success:
        print("\nğŸ‰ å¿«é€Ÿè¯„ä¼°æµ‹è¯•æˆåŠŸ!")
    else:
        print("\nğŸ’¥ å¿«é€Ÿè¯„ä¼°æµ‹è¯•å¤±è´¥!")
        exit(1)

if __name__ == "__main__":
    main()
