#!/usr/bin/env python3

"""
LoRAæ¨¡å‹è¯„ä¼°æµ‹è¯•è„šæœ¬
ç”¨äºå¿«é€Ÿæµ‹è¯•LoRAæ¨¡å‹æ˜¯å¦èƒ½æ­£å¸¸åŠ è½½å’Œæ¨ç†
"""

import argparse
import torch
import os
import json
from PIL import Image

def test_lora_model(model_path, base_model_path, test_image_path, test_question):
    """æµ‹è¯•LoRAæ¨¡å‹åŠ è½½å’Œæ¨ç†"""
    
    print("ğŸš€ å¼€å§‹LoRAæ¨¡å‹æµ‹è¯•...")
    print(f"æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"åŸºç¡€æ¨¡å‹: {base_model_path}")
    print(f"æµ‹è¯•å›¾ç‰‡: {test_image_path}")
    print(f"æµ‹è¯•é—®é¢˜: {test_question}")
    
    try:
        # è®¾ç½®ç¯å¢ƒ
        from mgm.model.builder import load_pretrained_model
        from mgm.utils import disable_torch_init
        from mgm.conversation import conv_templates
        from mgm.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN
        from mgm.mm_utils import tokenizer_image_token, get_model_name_from_path
        
        disable_torch_init()
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯LoRAæ¨¡å‹
        adapter_config_path = os.path.join(model_path, "adapter_config.json")
        if os.path.exists(adapter_config_path):
            print("âœ… æ£€æµ‹åˆ°LoRAæ¨¡å‹")
            
            # è¯»å–LoRAé…ç½®
            with open(adapter_config_path, 'r') as f:
                adapter_config = json.load(f)
            print(f"LoRAé…ç½®: r={adapter_config.get('r')}, alpha={adapter_config.get('lora_alpha')}")
            
            # ä½¿ç”¨PEFTåŠ è½½
            from peft import PeftModel
            
            # åŠ è½½åŸºç¡€æ¨¡å‹
            print("ğŸ“¥ åŠ è½½åŸºç¡€æ¨¡å‹...")
            model_name = get_model_name_from_path(base_model_path)
            tokenizer, base_model, image_processor, context_len = load_pretrained_model(
                base_model_path, None, model_name
            )
            print("âœ… åŸºç¡€æ¨¡å‹åŠ è½½æˆåŠŸ")
            
            # åŠ è½½LoRAé€‚é…å™¨
            print("ğŸ“¥ åŠ è½½LoRAé€‚é…å™¨...")
            model = PeftModel.from_pretrained(base_model, model_path)
            print("âœ… LoRAé€‚é…å™¨åŠ è½½æˆåŠŸ")
            
            # åˆå¹¶æƒé‡ï¼ˆå¯é€‰ï¼‰
            if hasattr(model, 'merge_and_unload'):
                print("ğŸ”„ åˆå¹¶LoRAæƒé‡...")
                model = model.merge_and_unload()
                print("âœ… æƒé‡åˆå¹¶å®Œæˆ")
            
        else:
            print("ğŸ“¥ æ ‡å‡†æ¨¡å‹åŠ è½½...")
            model_name = get_model_name_from_path(model_path)
            tokenizer, model, image_processor, context_len = load_pretrained_model(
                model_path, None, model_name
            )
        
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
        
        # æµ‹è¯•æ¨ç†
        if test_image_path and os.path.exists(test_image_path):
            print("ğŸ–¼ï¸  åŠ è½½æµ‹è¯•å›¾ç‰‡...")
            image = Image.open(test_image_path).convert('RGB')
            
            # å‡†å¤‡è¾“å…¥
            qs = test_question
            if model.config.mm_use_im_start_end:
                qs = DEFAULT_IMAGE_TOKEN + '\n' + qs
            else:
                qs = DEFAULT_IMAGE_TOKEN + '\n' + qs
            
            conv = conv_templates["gemma"].copy()
            conv.append_message(conv.roles[0], qs)
            conv.append_message(conv.roles[1], None)
            prompt = conv.get_prompt()
            
            # Tokenize
            input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()
            
            # å¤„ç†å›¾åƒ
            image_tensor = image_processor.preprocess(image, return_tensors='pt')['pixel_values'][0]
            
            print("ğŸ¤– å¼€å§‹æ¨ç†...")
            with torch.inference_mode():
                output_ids = model.generate(
                    input_ids,
                    images=image_tensor.unsqueeze(0).half().cuda(),
                    do_sample=False,
                    temperature=0,
                    max_new_tokens=512,
                    use_cache=True
                )
            
            # è§£ç è¾“å‡º
            input_token_len = input_ids.shape[1]
            outputs = tokenizer.batch_decode(output_ids[:, input_token_len:], skip_special_tokens=True)[0]
            outputs = outputs.strip()
            
            print("âœ… æ¨ç†å®Œæˆ!")
            print(f"ğŸ¯ é—®é¢˜: {test_question}")
            print(f"ğŸ¤– å›ç­”: {outputs}")
            
            return True, outputs
        else:
            print("âš ï¸  è·³è¿‡æ¨ç†æµ‹è¯•ï¼ˆæ— æµ‹è¯•å›¾ç‰‡ï¼‰")
            return True, "æ¨¡å‹åŠ è½½æˆåŠŸï¼Œä½†æœªè¿›è¡Œæ¨ç†æµ‹è¯•"
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False, str(e)

def main():
    parser = argparse.ArgumentParser(description='LoRAæ¨¡å‹æµ‹è¯•è„šæœ¬')
    parser.add_argument('--model-path', required=True, help='LoRAæ¨¡å‹è·¯å¾„')
    parser.add_argument('--base-model', required=True, help='åŸºç¡€æ¨¡å‹è·¯å¾„')
    parser.add_argument('--test-image', help='æµ‹è¯•å›¾ç‰‡è·¯å¾„')
    parser.add_argument('--test-question', default='What is in this image?', help='æµ‹è¯•é—®é¢˜')
    
    args = parser.parse_args()
    
    success, result = test_lora_model(
        args.model_path,
        args.base_model, 
        args.test_image,
        args.test_question
    )
    
    if success:
        print("\nğŸ‰ æµ‹è¯•æˆåŠŸ!")
        print(f"ç»“æœ: {result}")
    else:
        print(f"\nğŸ’¥ æµ‹è¯•å¤±è´¥: {result}")
        exit(1)

if __name__ == "__main__":
    main()
