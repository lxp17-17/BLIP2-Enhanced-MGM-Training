#!/bin/bash

# LoRAæœ€å°åŒ–æµ‹è¯•è„šæœ¬
# ç›®æ ‡: éªŒè¯LoRAé…ç½®æ˜¯å¦èƒ½è§£å†³æ˜¾å­˜é—®é¢˜

# ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„Pythonç¯å¢ƒ
export PYTHONPATH="$(cd "$(dirname "$0")" && pwd)/training:$PYTHONPATH"
export CUDA_VISIBLE_DEVICES=0
# ç¦ç”¨DeepSpeed CUDAæ‰©å±•ç¼–è¯‘ä»¥é¿å…ç‰ˆæœ¬ä¸åŒ¹é…
export DS_BUILD_OPS=0
export DS_SKIP_CUDA_CHECK=1
# è®¾ç½®CUDA_HOMEä»¥é¿å…CUDAç›¸å…³é”™è¯¯
export CUDA_HOME=/home/robot/lhp/miniconda3/envs/Syn0625
# è®¾ç½®PyTorchå†…å­˜ç®¡ç†ä»¥å‡å°‘å†…å­˜ç¢ç‰‡
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

############################################################################
########################### æµ‹è¯•å‚æ•°é…ç½® ###########################
############################################################################
# exp meta information
EXP_NAME=lora_test
SCRIPT_DIR=$(cd "$(dirname "$0")" && pwd)

# æµ‹è¯•å‚æ•° - æœ€å°åŒ–é…ç½®
FINETUNE_BATCH_SIZE_PER_GPU=1
FINETUNE_GRADIENT_ACCUMULATION_STEPS=128
FINETUNE_DATALOADER_NUM_WORKERS=1
LOGGING_STEP=1
CKPT_SAVE_STEPS=5        # 5æ­¥ä¿å­˜ä¸€æ¬¡
TOTAL_SAVE_CKPT_LIMIT=1
MAX_STEPS=10             # åªè®­ç»ƒ10æ­¥è¿›è¡Œæµ‹è¯•

# æ¨¡å‹å’Œæ•°æ®è·¯å¾„
FINETUNE_NAME=MGM-2B-Finetune-LoRA-Test
AUX_SIZE=768
NUM_TRAIN_EPOCHS=1

echo "ğŸš€ å¼€å§‹LoRAæœ€å°åŒ–æµ‹è¯•"
echo "é…ç½®å‚æ•°:"
echo "  - å®éªŒåç§°: $EXP_NAME"
echo "  - æœ€å¤§è®­ç»ƒæ­¥æ•°: $MAX_STEPS"
echo "  - Batch size: $FINETUNE_BATCH_SIZE_PER_GPU"
echo "  - Gradient accumulation: $FINETUNE_GRADIENT_ACCUMULATION_STEPS"
echo "  - LoRA rank: 16"
echo "  - LoRA alpha: 32"

# åˆ›å»ºè¾“å‡ºç›®å½•
mkdir -p $SCRIPT_DIR/../output/training_dirs/$FINETUNE_NAME

# æ‰§è¡Œå¾®è°ƒè®­ç»ƒ (è·³è¿‡é¢„è®­ç»ƒï¼Œç›´æ¥æµ‹è¯•å¾®è°ƒ)
echo "å¼€å§‹LoRAå¾®è°ƒæµ‹è¯•..."
echo "å‚æ•°:"
echo "  - Batch size per GPU: $FINETUNE_BATCH_SIZE_PER_GPU"
echo "  - Gradient accumulation steps: $FINETUNE_GRADIENT_ACCUMULATION_STEPS"
echo "  - Dataloader workers: $FINETUNE_DATALOADER_NUM_WORKERS"
echo "  - Max steps: $MAX_STEPS"
echo "  - DeepSpeed config: zero3.json (ZeRO Stage 3 for maximum memory efficiency)"

PYTHONPATH=$SCRIPT_DIR/training:$PYTHONPATH DS_BUILD_OPS=0 DS_SKIP_CUDA_CHECK=1 /home/robot/lhp/miniconda3/envs/Syn0625/bin/python $(which deepspeed) $SCRIPT_DIR/training/mgm/train/train_mem.py \
    --deepspeed $SCRIPT_DIR/training/scripts/zero3.json \
    --model_name_or_path $SCRIPT_DIR/training/model_zoo/LLM/gemma/gemma-2b-it \
    --version gemma \
    --data_path $SCRIPT_DIR/training/data/finetuning_stage_1_12k/mgm_instruction_stage_1_12k.json \
    --image_folder $SCRIPT_DIR/training/data/finetuning_stage_1_12k \
    --vision_tower $SCRIPT_DIR/training/model_zoo/OpenAI/clip-vit-large-patch14-336 \
    --vision_tower_aux $SCRIPT_DIR/training/model_zoo/OpenAI/openclip-convnext-large-d-320-laion2B-s29B-b131K-ft-soup \
    --pretrain_mm_mlp_adapter $SCRIPT_DIR/../output/training_dirs/MGM-2B-Pretrain-default/mm_projector.bin \
    --mm_projector_type mlp2x_gelu \
    --mm_vision_select_layer -2 \
    --mm_use_im_start_end False \
    --mm_use_im_patch_token False \
    --image_aspect_ratio pad \
    --group_by_modality_length True \
    --image_size_aux $AUX_SIZE \
    --fp16 True \
    --output_dir $SCRIPT_DIR/../output/training_dirs/$FINETUNE_NAME \
    --num_train_epochs $NUM_TRAIN_EPOCHS \
    --per_device_train_batch_size $FINETUNE_BATCH_SIZE_PER_GPU \
    --per_device_eval_batch_size 4 \
    --gradient_accumulation_steps $FINETUNE_GRADIENT_ACCUMULATION_STEPS \
    --evaluation_strategy "no" \
    --save_strategy "steps" \
    --save_steps $CKPT_SAVE_STEPS \
    --save_total_limit $TOTAL_SAVE_CKPT_LIMIT \
    --learning_rate 2e-5 \
    --weight_decay 0. \
    --warmup_ratio 0.03 \
    --lr_scheduler_type "cosine" \
    --logging_steps $LOGGING_STEP \
    --tf32 True \
    --model_max_length 1024 \
    --gradient_checkpointing True \
    --dataloader_num_workers $FINETUNE_DATALOADER_NUM_WORKERS \
    --lazy_preprocess True \
    --report_to none \
    --max_steps $MAX_STEPS \
    --lora_enable True \
    --lora_r 16 \
    --lora_alpha 32 \
    --lora_dropout 0.1 \
    --lora_bias none \
    2>&1 | tee $SCRIPT_DIR/../output/training_dirs/$FINETUNE_NAME/lora_test.log

# æ£€æŸ¥è®­ç»ƒç»“æœ
if [ $? -eq 0 ]; then
    echo "âœ… LoRAæµ‹è¯•æˆåŠŸå®Œæˆ!"
    echo "ğŸ“Š æ£€æŸ¥æ˜¾å­˜ä½¿ç”¨æƒ…å†µ..."
    nvidia-smi
    echo "ğŸ“ æ£€æŸ¥è¾“å‡ºæ–‡ä»¶..."
    ls -la $SCRIPT_DIR/../output/training_dirs/$FINETUNE_NAME/
else
    echo "âŒ LoRAæµ‹è¯•å¤±è´¥"
    echo "ğŸ“‹ æŸ¥çœ‹é”™è¯¯æ—¥å¿—:"
    tail -20 $SCRIPT_DIR/../output/training_dirs/$FINETUNE_NAME/lora_test.log
fi

echo "ğŸ LoRAæœ€å°åŒ–æµ‹è¯•å®Œæˆ"
