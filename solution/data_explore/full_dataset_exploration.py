#!/usr/bin/env python3
"""
å®Œæ•´ç§å­æ•°æ®é›†æ·±åº¦æ¢ç´¢è„šæœ¬ - 40ä¸‡æ•°æ®åˆ†æ
å…¨é¢åˆ†æ40ä¸‡å®Œæ•´ç§å­æ•°æ®é›†çš„ç‰¹å¾ï¼Œç”Ÿæˆè¯¦ç»†çš„å›¾æ–‡åˆ†ææŠ¥å‘Š
æ”¯æŒé‡‡æ ·åˆ†æã€å¯è§†åŒ–å›¾è¡¨ç”Ÿæˆå’Œç»Ÿè®¡æŠ¥å‘Š
"""

import json
import os
import re
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter, defaultdict
from pathlib import Path
import pandas as pd
from PIL import Image
import hashlib
import random
import time
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“å’Œæ ·å¼
plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial']
plt.rcParams['axes.unicode_minus'] = False
plt.style.use('default')
sns.set_palette("husl")

class FullDatasetExplorer:
    def __init__(self, sample_size=50000):
        """
        åˆå§‹åŒ–å®Œæ•´æ•°æ®é›†æ¢ç´¢å™¨
        Args:
            sample_size: é‡‡æ ·å¤§å°ï¼Œç”¨äºå›¾åƒåˆ†æç­‰è€—æ—¶æ“ä½œ
        """
        self.data = []
        self.sample_data = []
        self.sample_size = sample_size
        self.analysis_results = {}
        self.output_dir = Path("output/full_dataset_analysis")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # è®¾ç½®å›¾è¡¨ä¿å­˜ç›®å½•
        self.charts_dir = self.output_dir / "charts"
        self.charts_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸ” åˆå§‹åŒ–å®Œæ•´æ•°æ®é›†æ¢ç´¢å™¨")
        print(f"ğŸ“Š é‡‡æ ·å¤§å°: {sample_size:,}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")

    def load_full_dataset(self):
        """åŠ è½½å®Œæ•´çš„40ä¸‡æ•°æ®é›†"""
        print("ğŸ” åŠ è½½å®Œæ•´ç§å­æ•°æ®é›†...")
        
        data_file = "input/pretrain_stage_1/mgm_pretrain_stage_1.jsonl"
        
        if not os.path.exists(data_file):
            print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
            return False
        
        print(f"ğŸ“Š å¼€å§‹åŠ è½½æ•°æ®æ–‡ä»¶: {data_file}")
        start_time = time.time()
        
        # ä½¿ç”¨è¿›åº¦æ¡åŠ è½½æ•°æ®
        with open(data_file, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(tqdm(f, desc="åŠ è½½æ•°æ®"), 1):
                if line.strip():
                    try:
                        item = json.loads(line.strip())
                        item['_line_number'] = line_num
                        self.data.append(item)
                    except json.JSONDecodeError as e:
                        print(f"âš ï¸ ç¬¬{line_num}è¡ŒJSONè§£æé”™è¯¯: {e}")
                        continue
        
        load_time = time.time() - start_time
        print(f"âœ… æˆåŠŸåŠ è½½ {len(self.data):,} æ¡æ•°æ®ï¼Œè€—æ—¶: {load_time:.1f}ç§’")
        
        # åˆ›å»ºé‡‡æ ·æ•°æ®ç”¨äºå›¾åƒåˆ†æ
        if len(self.data) > self.sample_size:
            print(f"ğŸ² åˆ›å»º {self.sample_size:,} æ ·æœ¬çš„éšæœºé‡‡æ ·...")
            self.sample_data = random.sample(self.data, self.sample_size)
        else:
            self.sample_data = self.data.copy()
            
        print(f"ğŸ“ˆ é‡‡æ ·æ•°æ®å¤§å°: {len(self.sample_data):,}")
        return True

    def analyze_data_structure(self):
        """åˆ†ææ•°æ®ç»“æ„"""
        print("\n=== æ•°æ®ç»“æ„åˆ†æ ===")
        
        if not self.data:
            return
        
        # åˆ†æå­—æ®µåˆ†å¸ƒ
        field_counts = defaultdict(int)
        field_types = defaultdict(set)
        
        # ä½¿ç”¨é‡‡æ ·æ•°æ®è¿›è¡Œå­—æ®µåˆ†æ
        sample_for_structure = self.data[:10000]  # ä½¿ç”¨å‰1ä¸‡æ¡è¿›è¡Œç»“æ„åˆ†æ
        
        for item in tqdm(sample_for_structure, desc="åˆ†ææ•°æ®ç»“æ„"):
            for key, value in item.items():
                field_counts[key] += 1
                field_types[key].add(type(value).__name__)
        
        print("ğŸ“‹ å­—æ®µç»Ÿè®¡:")
        for field, count in sorted(field_counts.items()):
            coverage = count / len(sample_for_structure) * 100
            types = ', '.join(field_types[field])
            print(f"  {field}: {count:,}/{len(sample_for_structure):,} ({coverage:.1f}%) - ç±»å‹: {types}")
        
        # æ£€æŸ¥å¿…è¦å­—æ®µ
        required_fields = ['text', 'images', 'id']
        missing_fields = []
        
        for field in required_fields:
            if field not in field_counts or field_counts[field] < len(sample_for_structure):
                missing_fields.append(field)
        
        if missing_fields:
            print(f"âš ï¸ ç¼ºå¤±å¿…è¦å­—æ®µ: {missing_fields}")
        else:
            print("âœ… æ‰€æœ‰å¿…è¦å­—æ®µå®Œæ•´")
        
        self.analysis_results['data_structure'] = {
            'total_samples': len(self.data),
            'analyzed_samples': len(sample_for_structure),
            'field_counts': dict(field_counts),
            'field_types': {k: list(v) for k, v in field_types.items()},
            'missing_fields': missing_fields
        }

    def analyze_text_features_comprehensive(self):
        """å…¨é¢åˆ†ææ–‡æœ¬ç‰¹å¾"""
        print("\n=== æ–‡æœ¬ç‰¹å¾å…¨é¢åˆ†æ ===")
        
        texts = []
        original_texts = []
        
        print("ğŸ“ æå–æ–‡æœ¬æ•°æ®...")
        for item in tqdm(self.data, desc="æå–æ–‡æœ¬"):
            original_text = item.get('text', '')
            original_texts.append(original_text)
            
            # æ¸…ç†Data-Juicerçš„ç‰¹æ®Šæ ‡è®°
            clean_text = original_text.replace('<__dj__image>', '').replace('<|__dj__eoc|>', '').strip()
            texts.append(clean_text)
        
        # åŸºç¡€ç»Ÿè®¡
        print("ğŸ“Š è®¡ç®—åŸºç¡€ç»Ÿè®¡...")
        word_lengths = [len(text.split()) for text in texts if text]
        char_lengths = [len(text) for text in texts if text]
        sentence_counts = [len(re.split(r'[.!?]+', text)) for text in texts if text]
        
        print(f"ğŸ“Š åŸºç¡€ç»Ÿè®¡ (æ€»æ ·æœ¬: {len(texts):,}):")
        print(f"  å¹³å‡è¯æ•°: {np.mean(word_lengths):.2f} (ä¸­ä½æ•°: {np.median(word_lengths):.2f})")
        print(f"  è¯æ•°èŒƒå›´: {min(word_lengths)} - {max(word_lengths)}")
        print(f"  å¹³å‡å­—ç¬¦æ•°: {np.mean(char_lengths):.2f}")
        print(f"  å¹³å‡å¥å­æ•°: {np.mean(sentence_counts):.2f}")
        
        # ç”Ÿæˆè¯é•¿åˆ†å¸ƒå›¾
        self.create_text_length_charts(word_lengths, char_lengths)
        
        # è´¨é‡åˆ†æ
        empty_texts = len([t for t in texts if not t])
        short_texts = len([l for l in word_lengths if l <= 3])
        long_texts = len([l for l in word_lengths if l >= 50])
        
        print(f"\nğŸ“‹ è´¨é‡åˆ†æ:")
        print(f"  ç©ºæ–‡æœ¬: {empty_texts:,} ({empty_texts/len(texts)*100:.1f}%)")
        print(f"  è¿‡çŸ­æ–‡æœ¬(â‰¤3è¯): {short_texts:,} ({short_texts/len(word_lengths)*100:.1f}%)")
        print(f"  è¿‡é•¿æ–‡æœ¬(â‰¥50è¯): {long_texts:,} ({long_texts/len(word_lengths)*100:.1f}%)")
        
        # è¯é¢‘åˆ†æ (ä½¿ç”¨é‡‡æ ·æ•°æ®)
        print("ğŸ“ˆ è¿›è¡Œè¯é¢‘åˆ†æ...")
        sample_texts = random.sample(texts, min(50000, len(texts)))
        all_words = []
        for text in tqdm(sample_texts, desc="è¯é¢‘åˆ†æ"):
            words = re.findall(r'\b[a-zA-Z]+\b', text.lower())
            all_words.extend(words)
        
        word_freq = Counter(all_words)
        print(f"\nğŸ“ˆ è¯é¢‘åˆ†æ (åŸºäº {len(sample_texts):,} æ ·æœ¬):")
        print(f"  æ€»è¯æ±‡: {len(all_words):,}, å”¯ä¸€è¯æ±‡: {len(word_freq):,}")
        print("  æœ€å¸¸è§çš„15ä¸ªè¯:")
        for word, count in word_freq.most_common(15):
            print(f"    {word}: {count:,} ({count/len(all_words)*100:.2f}%)")
        
        # ç”Ÿæˆè¯é¢‘å›¾è¡¨
        self.create_word_frequency_chart(word_freq)
        
        # ä¿å­˜åˆ†æç»“æœ
        self.analysis_results['text_features'] = {
            'total_texts': len(texts),
            'avg_word_length': float(np.mean(word_lengths)),
            'median_word_length': float(np.median(word_lengths)),
            'std_word_length': float(np.std(word_lengths)),
            'avg_char_length': float(np.mean(char_lengths)),
            'avg_sentence_count': float(np.mean(sentence_counts)),
            'empty_texts': empty_texts,
            'short_texts': short_texts,
            'long_texts': long_texts,
            'unique_words': len(word_freq),
            'total_words_analyzed': len(all_words),
            'top_words': word_freq.most_common(50),
            'word_length_percentiles': {
                '25th': float(np.percentile(word_lengths, 25)),
                '50th': float(np.percentile(word_lengths, 50)),
                '75th': float(np.percentile(word_lengths, 75)),
                '90th': float(np.percentile(word_lengths, 90)),
                '95th': float(np.percentile(word_lengths, 95))
            }
        }

    def create_text_length_charts(self, word_lengths, char_lengths):
        """åˆ›å»ºæ–‡æœ¬é•¿åº¦åˆ†å¸ƒå›¾è¡¨"""
        print("ğŸ“Š ç”Ÿæˆæ–‡æœ¬é•¿åº¦åˆ†å¸ƒå›¾è¡¨...")
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
        
        # è¯æ•°åˆ†å¸ƒç›´æ–¹å›¾
        ax1.hist(word_lengths, bins=50, alpha=0.7, color='skyblue', edgecolor='black')
        ax1.set_title('Word Count Distribution', fontsize=14, fontweight='bold')
        ax1.set_xlabel('Number of Words')
        ax1.set_ylabel('Frequency')
        ax1.axvline(np.mean(word_lengths), color='red', linestyle='--', 
                   label=f'Mean: {np.mean(word_lengths):.1f}')
        ax1.legend()
        
        # è¯æ•°åˆ†å¸ƒç®±çº¿å›¾
        ax2.boxplot(word_lengths, vert=True)
        ax2.set_title('Word Count Box Plot', fontsize=14, fontweight='bold')
        ax2.set_ylabel('Number of Words')
        
        # å­—ç¬¦æ•°åˆ†å¸ƒç›´æ–¹å›¾
        ax3.hist(char_lengths, bins=50, alpha=0.7, color='lightgreen', edgecolor='black')
        ax3.set_title('Character Count Distribution', fontsize=14, fontweight='bold')
        ax3.set_xlabel('Number of Characters')
        ax3.set_ylabel('Frequency')
        ax3.axvline(np.mean(char_lengths), color='red', linestyle='--',
                   label=f'Mean: {np.mean(char_lengths):.1f}')
        ax3.legend()
        
        # è¯æ•°vså­—ç¬¦æ•°æ•£ç‚¹å›¾
        sample_indices = random.sample(range(len(word_lengths)), min(10000, len(word_lengths)))
        sample_words = [word_lengths[i] for i in sample_indices]
        sample_chars = [char_lengths[i] for i in sample_indices]
        
        ax4.scatter(sample_words, sample_chars, alpha=0.5, s=1)
        ax4.set_title('Words vs Characters Correlation', fontsize=14, fontweight='bold')
        ax4.set_xlabel('Number of Words')
        ax4.set_ylabel('Number of Characters')
        
        plt.tight_layout()
        plt.savefig(self.charts_dir / 'text_length_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… æ–‡æœ¬é•¿åº¦åˆ†æå›¾è¡¨å·²ä¿å­˜: {self.charts_dir / 'text_length_analysis.png'}")

    def create_word_frequency_chart(self, word_freq):
        """åˆ›å»ºè¯é¢‘åˆ†æå›¾è¡¨"""
        print("ğŸ“Š ç”Ÿæˆè¯é¢‘åˆ†æå›¾è¡¨...")
        
        # è·å–å‰30ä¸ªæœ€å¸¸è§è¯æ±‡
        top_words = word_freq.most_common(30)
        words, counts = zip(*top_words)
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))
        
        # è¯é¢‘æ¡å½¢å›¾
        bars = ax1.bar(range(len(words)), counts, color='coral', alpha=0.8)
        ax1.set_title('Top 30 Most Frequent Words', fontsize=16, fontweight='bold')
        ax1.set_xlabel('Words')
        ax1.set_ylabel('Frequency')
        ax1.set_xticks(range(len(words)))
        ax1.set_xticklabels(words, rotation=45, ha='right')
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, count in zip(bars, counts):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                    f'{count:,}', ha='center', va='bottom', fontsize=8)
        
        # è¯é¢‘åˆ†å¸ƒå¯¹æ•°å›¾
        all_counts = list(word_freq.values())
        ax2.hist(all_counts, bins=50, alpha=0.7, color='lightblue', edgecolor='black')
        ax2.set_title('Word Frequency Distribution (Log Scale)', fontsize=16, fontweight='bold')
        ax2.set_xlabel('Frequency')
        ax2.set_ylabel('Number of Words')
        ax2.set_yscale('log')
        ax2.set_xscale('log')
        
        plt.tight_layout()
        plt.savefig(self.charts_dir / 'word_frequency_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… è¯é¢‘åˆ†æå›¾è¡¨å·²ä¿å­˜: {self.charts_dir / 'word_frequency_analysis.png'}")

    def analyze_image_features_sampled(self):
        """åŸºäºé‡‡æ ·æ•°æ®åˆ†æå›¾åƒç‰¹å¾"""
        print(f"\n=== å›¾åƒç‰¹å¾åˆ†æ (åŸºäº {len(self.sample_data):,} æ ·æœ¬) ===")

        image_paths = []
        missing_images = 0
        image_sizes = []
        image_formats = []

        print("ğŸ–¼ï¸ åˆ†æå›¾åƒç‰¹å¾...")
        for item in tqdm(self.sample_data, desc="åˆ†æå›¾åƒ"):
            if 'images' in item and item['images']:
                img_path = item['images'][0]
                image_paths.append(img_path)

                # æ„å»ºå®Œæ•´çš„å›¾åƒè·¯å¾„
                full_img_path = os.path.join('input/pretrain_stage_1', img_path)

                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                if os.path.exists(full_img_path):
                    try:
                        # è·å–å›¾åƒä¿¡æ¯
                        with Image.open(full_img_path) as img:
                            width, height = img.size
                            image_sizes.append((width, height))
                            image_formats.append(img.format)
                    except Exception as e:
                        missing_images += 1
                else:
                    missing_images += 1

        print(f"ğŸ“Š å›¾åƒåŸºç¡€ç»Ÿè®¡:")
        print(f"  å›¾åƒæ€»æ•°: {len(image_paths):,}")
        print(f"  ç¼ºå¤±å›¾åƒ: {missing_images:,}")
        print(f"  å›¾åƒå®Œæ•´ç‡: {(len(image_paths)-missing_images)/len(image_paths)*100:.1f}%")

        if image_sizes:
            # å°ºå¯¸åˆ†æ
            widths = [size[0] for size in image_sizes]
            heights = [size[1] for size in image_sizes]
            aspect_ratios = [w/h for w, h in image_sizes]

            print(f"\nğŸ“ å°ºå¯¸åˆ†æ:")
            print(f"  å¹³å‡å®½åº¦: {np.mean(widths):.1f}px (èŒƒå›´: {min(widths)}-{max(widths)})")
            print(f"  å¹³å‡é«˜åº¦: {np.mean(heights):.1f}px (èŒƒå›´: {min(heights)}-{max(heights)})")
            print(f"  å¹³å‡å®½é«˜æ¯”: {np.mean(aspect_ratios):.2f}")

            # ç”Ÿæˆå›¾åƒåˆ†æå›¾è¡¨
            self.create_image_analysis_charts(widths, heights, aspect_ratios, image_formats)

            # å°ºå¯¸åˆ†å¸ƒ
            small_images = len([w for w in widths if w < 224])
            medium_images = len([w for w in widths if 224 <= w < 512])
            large_images = len([w for w in widths if w >= 512])

            print(f"\nğŸ“Š å°ºå¯¸åˆ†å¸ƒ:")
            print(f"  å°å›¾åƒ(<224px): {small_images:,} ({small_images/len(widths)*100:.1f}%)")
            print(f"  ä¸­ç­‰å›¾åƒ(224-512px): {medium_images:,} ({medium_images/len(widths)*100:.1f}%)")
            print(f"  å¤§å›¾åƒ(â‰¥512px): {large_images:,} ({large_images/len(widths)*100:.1f}%)")

        # æ ¼å¼åˆ†æ
        if image_formats:
            format_counts = Counter(image_formats)
            print(f"\nğŸ¨ æ ¼å¼åˆ†æ:")
            for fmt, count in format_counts.most_common():
                print(f"  {fmt}: {count:,} ({count/len(image_formats)*100:.1f}%)")

        # ä¿å­˜åˆ†æç»“æœ
        self.analysis_results['image_features'] = {
            'total_images_sampled': len(image_paths),
            'missing_images': missing_images,
            'completion_rate': (len(image_paths)-missing_images)/len(image_paths)*100 if image_paths else 0,
            'avg_width': float(np.mean(widths)) if widths else 0,
            'avg_height': float(np.mean(heights)) if heights else 0,
            'avg_aspect_ratio': float(np.mean(aspect_ratios)) if aspect_ratios else 0,
            'size_distribution': {
                'small': small_images if image_sizes else 0,
                'medium': medium_images if image_sizes else 0,
                'large': large_images if image_sizes else 0
            } if image_sizes else {},
            'format_distribution': dict(format_counts) if image_formats else {},
            'sample_size': len(self.sample_data)
        }

    def create_image_analysis_charts(self, widths, heights, aspect_ratios, image_formats):
        """åˆ›å»ºå›¾åƒåˆ†æå›¾è¡¨"""
        print("ğŸ“Š ç”Ÿæˆå›¾åƒåˆ†æå›¾è¡¨...")

        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))

        # å›¾åƒå®½åº¦åˆ†å¸ƒ
        ax1.hist(widths, bins=50, alpha=0.7, color='lightblue', edgecolor='black')
        ax1.set_title('Image Width Distribution', fontsize=14, fontweight='bold')
        ax1.set_xlabel('Width (pixels)')
        ax1.set_ylabel('Frequency')
        ax1.axvline(np.mean(widths), color='red', linestyle='--',
                   label=f'Mean: {np.mean(widths):.0f}px')
        ax1.legend()

        # å›¾åƒé«˜åº¦åˆ†å¸ƒ
        ax2.hist(heights, bins=50, alpha=0.7, color='lightgreen', edgecolor='black')
        ax2.set_title('Image Height Distribution', fontsize=14, fontweight='bold')
        ax2.set_xlabel('Height (pixels)')
        ax2.set_ylabel('Frequency')
        ax2.axvline(np.mean(heights), color='red', linestyle='--',
                   label=f'Mean: {np.mean(heights):.0f}px')
        ax2.legend()

        # å®½é«˜æ¯”åˆ†å¸ƒ
        ax3.hist(aspect_ratios, bins=50, alpha=0.7, color='coral', edgecolor='black')
        ax3.set_title('Aspect Ratio Distribution', fontsize=14, fontweight='bold')
        ax3.set_xlabel('Aspect Ratio (Width/Height)')
        ax3.set_ylabel('Frequency')
        ax3.axvline(np.mean(aspect_ratios), color='red', linestyle='--',
                   label=f'Mean: {np.mean(aspect_ratios):.2f}')
        ax3.legend()

        # å›¾åƒæ ¼å¼åˆ†å¸ƒ
        format_counts = Counter(image_formats)
        formats, counts = zip(*format_counts.most_common())
        ax4.pie(counts, labels=formats, autopct='%1.1f%%', startangle=90)
        ax4.set_title('Image Format Distribution', fontsize=14, fontweight='bold')

        plt.tight_layout()
        plt.savefig(self.charts_dir / 'image_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()

        print(f"âœ… å›¾åƒåˆ†æå›¾è¡¨å·²ä¿å­˜: {self.charts_dir / 'image_analysis.png'}")

    def generate_comprehensive_report(self):
        """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
        print("\nğŸ“ ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š...")

        import time
        current_time = time.strftime('%Y-%m-%d %H:%M:%S')

        # è·å–åˆ†æç»“æœ
        data_structure = self.analysis_results.get('data_structure', {})
        text_features = self.analysis_results.get('text_features', {})
        image_features = self.analysis_results.get('image_features', {})

        report_content = f"""# å®Œæ•´ç§å­æ•°æ®é›†æ·±åº¦åˆ†ææŠ¥å‘Š

## æŠ¥å‘Šä¿¡æ¯
- **ç”Ÿæˆæ—¶é—´**: {current_time}
- **æ•°æ®é›†**: å®Œæ•´40ä¸‡ç§å­æ•°æ®é›† (pretrain_stage_1)
- **æ€»æ ·æœ¬æ•°**: {data_structure.get('total_samples', 0):,}
- **å›¾åƒåˆ†æé‡‡æ ·**: {image_features.get('sample_size', 0):,} æ ·æœ¬

## ğŸ¯ æ‰§è¡Œæ‘˜è¦

### æ•°æ®è§„æ¨¡
- **æ€»æ•°æ®é‡**: {data_structure.get('total_samples', 0):,} æ¡è®°å½•
- **æ•°æ®å®Œæ•´æ€§**: {'âœ… å®Œæ•´' if not data_structure.get('missing_fields') else 'âš ï¸ æœ‰ç¼ºå¤±'}
- **é¢„ä¼°å¤„ç†æ—¶é—´**: {self._estimate_processing_time(data_structure.get('total_samples', 0))}

### å…³é”®å‘ç°
1. **æ–‡æœ¬ç‰¹å¾**: å¹³å‡ {text_features.get('avg_word_length', 0):.1f} è¯/æ ·æœ¬
2. **å›¾åƒè´¨é‡**: {image_features.get('completion_rate', 0):.1f}% å®Œæ•´ç‡
3. **è¯æ±‡å¤šæ ·æ€§**: {text_features.get('unique_words', 0):,} å”¯ä¸€è¯æ±‡

## ğŸ“Š è¯¦ç»†åˆ†æ

### 1. æ•°æ®ç»“æ„æ¦‚è§ˆ

#### åŸºç¡€ç»Ÿè®¡
- **æ€»æ ·æœ¬æ•°**: {data_structure.get('total_samples', 0):,}
- **å­—æ®µå®Œæ•´æ€§**: {'âœ… å®Œæ•´' if not data_structure.get('missing_fields') else 'âš ï¸ æœ‰ç¼ºå¤±'}

#### å­—æ®µåˆ†å¸ƒ
{self._format_field_distribution(data_structure.get('field_counts', {}))}

### 2. æ–‡æœ¬ç‰¹å¾æ·±åº¦åˆ†æ

#### é•¿åº¦ç»Ÿè®¡
- **å¹³å‡è¯æ•°**: {text_features.get('avg_word_length', 0):.2f} Â± {text_features.get('std_word_length', 0):.2f}
- **ä¸­ä½æ•°è¯æ•°**: {text_features.get('median_word_length', 0):.1f}
- **å¹³å‡å­—ç¬¦æ•°**: {text_features.get('avg_char_length', 0):.1f}

#### åˆ†å¸ƒç‰¹å¾
- **25thç™¾åˆ†ä½**: {text_features.get('word_length_percentiles', {}).get('25th', 0):.1f} è¯
- **75thç™¾åˆ†ä½**: {text_features.get('word_length_percentiles', {}).get('75th', 0):.1f} è¯
- **90thç™¾åˆ†ä½**: {text_features.get('word_length_percentiles', {}).get('90th', 0):.1f} è¯

#### è´¨é‡åˆ†å¸ƒ
- **ç©ºæ–‡æœ¬**: {text_features.get('empty_texts', 0):,} ({text_features.get('empty_texts', 0)/data_structure.get('total_samples', 1)*100:.2f}%)
- **è¿‡çŸ­æ–‡æœ¬(â‰¤3è¯)**: {text_features.get('short_texts', 0):,} ({text_features.get('short_texts', 0)/data_structure.get('total_samples', 1)*100:.2f}%)
- **è¿‡é•¿æ–‡æœ¬(â‰¥50è¯)**: {text_features.get('long_texts', 0):,} ({text_features.get('long_texts', 0)/data_structure.get('total_samples', 1)*100:.2f}%)

#### è¯æ±‡ç»Ÿè®¡
- **æ€»è¯æ±‡æ•°**: {text_features.get('total_words_analyzed', 0):,}
- **å”¯ä¸€è¯æ±‡æ•°**: {text_features.get('unique_words', 0):,}
- **è¯æ±‡å¤šæ ·æ€§**: {text_features.get('unique_words', 0)/max(text_features.get('total_words_analyzed', 1), 1):.4f}

### 3. å›¾åƒç‰¹å¾åˆ†æ (åŸºäº {image_features.get('sample_size', 0):,} æ ·æœ¬)

#### åŸºç¡€ç»Ÿè®¡
- **å›¾åƒå®Œæ•´ç‡**: {image_features.get('completion_rate', 0):.1f}%
- **å¹³å‡å°ºå¯¸**: {image_features.get('avg_width', 0):.0f}Ã—{image_features.get('avg_height', 0):.0f}px
- **å¹³å‡å®½é«˜æ¯”**: {image_features.get('avg_aspect_ratio', 0):.2f}

#### å°ºå¯¸åˆ†å¸ƒ
{self._format_size_distribution(image_features.get('size_distribution', {}))}

#### æ ¼å¼åˆ†å¸ƒ
{self._format_format_distribution(image_features.get('format_distribution', {}))}

## ğŸ¯ æ•°æ®åˆæˆå»ºè®®

### æ ¸å¿ƒç­–ç•¥
åŸºäºåˆ†æç»“æœï¼Œæ¨èä»¥ä¸‹Data-Juicerå¤„ç†ç­–ç•¥ï¼š

1. **æ–‡æœ¬å¢å¼ºä¼˜å…ˆçº§**: ğŸ”¥ğŸ”¥ğŸ”¥ (å¹³å‡è¯æ•°è¾ƒå°‘ï¼Œéœ€è¦å¤§å¹…å¢å¼º)
2. **å›¾åƒè´¨é‡æ§åˆ¶**: ğŸ”¥ğŸ”¥ (è´¨é‡è‰¯å¥½ï¼Œé€‚åº¦è¿‡æ»¤å³å¯)
3. **å¤šæ¨¡æ€å¯¹é½**: ğŸ”¥ğŸ”¥ğŸ”¥ (ç¡®ä¿å›¾æ–‡åŒ¹é…åº¦)

### æ¨èé…ç½®å‚æ•°
```yaml
# åŸºäº40ä¸‡æ•°æ®åˆ†æçš„æ¨èé…ç½®
text_length_filter:
  min_len: {max(5, int(text_features.get('avg_word_length', 10) * 0.5))}
  max_len: {min(200, int(text_features.get('avg_word_length', 10) * 5))}

image_shape_filter:
  min_width: {max(224, int(image_features.get('avg_width', 300) * 0.6))}
  min_height: {max(224, int(image_features.get('avg_height', 300) * 0.6))}

image_text_similarity_filter:
  min_score: 0.2  # é€‚åº”å½“å‰æ–‡æœ¬ç®€å•çš„ç‰¹ç‚¹
```

### å¤„ç†ä¼˜å…ˆçº§
1. **ç«‹å³æ‰§è¡Œ**: æ–‡æœ¬ä¸°å¯ŒåŒ– (BLIP2å¤šæ¨¡å‹æè¿°ç”Ÿæˆ)
2. **é‡ç‚¹å…³æ³¨**: è¯æ±‡å¤šæ ·æ€§æå‡
3. **è´¨é‡æ§åˆ¶**: å¤šæ¨¡æ€å¯¹é½éªŒè¯

## ğŸ“ˆ é¢„æœŸæ•ˆæœ

### æ•°æ®å¢å¼ºç›®æ ‡
- **æ–‡æœ¬é•¿åº¦**: {text_features.get('avg_word_length', 0):.1f} â†’ 15-20 è¯ (æå‡ {(18/max(text_features.get('avg_word_length', 1), 1) - 1)*100:.0f}%)
- **è¯æ±‡å¤šæ ·æ€§**: å½“å‰ {text_features.get('unique_words', 0)/max(text_features.get('total_words_analyzed', 1), 1):.3f} â†’ ç›®æ ‡ 0.200+
- **æ ·æœ¬è´¨é‡**: é¢„æœŸæå‡ 30-50%

### å¤„ç†æ—¶é—´é¢„ä¼°
- **é¢„å¤„ç†é˜¶æ®µ**: ~2-4å°æ—¶
- **BLIP2æè¿°ç”Ÿæˆ**: ~8-12å°æ—¶
- **è´¨é‡æ§åˆ¶**: ~1-2å°æ—¶
- **æ€»è®¡**: ~12-18å°æ—¶

## ğŸ“‹ ä¸‹ä¸€æ­¥è¡ŒåŠ¨è®¡åˆ’

### Phase 1: ç«‹å³æ‰§è¡Œ (ä»Šå¤©)
1. âœ… å®Œæˆæ•°æ®æ¢ç´¢åˆ†æ
2. ğŸ”„ ä¼˜åŒ–Data-Juiceré…ç½®
3. ğŸš€ å¯åŠ¨æ•°æ®å¤„ç†æµç¨‹

### Phase 2: ç›‘æ§ä¸ä¼˜åŒ– (æ˜å¤©)
1. ğŸ“Š ç›‘æ§å¤„ç†è¿›åº¦
2. ğŸ” ä¸­æœŸè´¨é‡æ£€æŸ¥
3. âš™ï¸ å‚æ•°è°ƒä¼˜

### Phase 3: éªŒè¯ä¸è®­ç»ƒ (åå¤©)
1. âœ… éªŒè¯åˆæˆæ•°æ®è´¨é‡
2. ğŸš€ å¯åŠ¨å®Œæ•´MGMè®­ç»ƒ
3. ğŸ“ˆ æ€§èƒ½å¯¹æ¯”åˆ†æ

---

## ğŸ“Š å¯è§†åŒ–å›¾è¡¨

æœ¬æŠ¥å‘ŠåŒ…å«ä»¥ä¸‹å¯è§†åŒ–åˆ†æå›¾è¡¨ï¼š
- ğŸ“ˆ `charts/text_length_analysis.png` - æ–‡æœ¬é•¿åº¦åˆ†å¸ƒåˆ†æ
- ğŸ“Š `charts/word_frequency_analysis.png` - è¯é¢‘åˆ†æ
- ğŸ–¼ï¸ `charts/image_analysis.png` - å›¾åƒç‰¹å¾åˆ†æ

---
*æŠ¥å‘Šç”±å®Œæ•´æ•°æ®é›†æ¢ç´¢å™¨è‡ªåŠ¨ç”Ÿæˆ - {current_time}*
"""

        # ä¿å­˜æŠ¥å‘Š
        report_path = self.output_dir / "comprehensive_analysis_report.md"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)

        # ä¿å­˜JSONæ ¼å¼çš„åˆ†æç»“æœ
        json_path = self.output_dir / "analysis_results.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(self.analysis_results, f, indent=2, ensure_ascii=False)

        print(f"âœ… ç»¼åˆåˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
        print(f"âœ… åˆ†æç»“æœJSONå·²ä¿å­˜åˆ°: {json_path}")
        print(f"ğŸ“Š å›¾è¡¨ç›®å½•: {self.charts_dir}")

    def _estimate_processing_time(self, total_samples):
        """ä¼°ç®—å¤„ç†æ—¶é—´"""
        # åŸºäºç»éªŒä¼°ç®—ï¼šæ¯1000æ ·æœ¬çº¦éœ€1-2åˆ†é’Ÿ
        minutes = (total_samples / 1000) * 1.5
        if minutes < 60:
            return f"çº¦ {minutes:.0f} åˆ†é’Ÿ"
        else:
            hours = minutes / 60
            return f"çº¦ {hours:.1f} å°æ—¶"

    def _format_field_distribution(self, field_counts):
        """æ ¼å¼åŒ–å­—æ®µåˆ†å¸ƒ"""
        lines = []
        for field, count in sorted(field_counts.items()):
            lines.append(f"- **{field}**: {count:,}")
        return '\n'.join(lines)

    def _format_size_distribution(self, size_dist):
        """æ ¼å¼åŒ–å°ºå¯¸åˆ†å¸ƒ"""
        total = sum(size_dist.values()) if size_dist else 1
        lines = []
        for size_type, count in size_dist.items():
            percentage = count / total * 100
            lines.append(f"- **{size_type}**: {count:,} ({percentage:.1f}%)")
        return '\n'.join(lines)

    def _format_format_distribution(self, format_dist):
        """æ ¼å¼åŒ–æ ¼å¼åˆ†å¸ƒ"""
        total = sum(format_dist.values()) if format_dist else 1
        lines = []
        for fmt, count in format_dist.items():
            percentage = count / total * 100
            lines.append(f"- **{fmt}**: {count:,} ({percentage:.1f}%)")
        return '\n'.join(lines)

    def run_full_exploration(self):
        """è¿è¡Œå®Œæ•´çš„æ•°æ®æ¢ç´¢æµç¨‹"""
        print("ğŸ¯ å¼€å§‹å®Œæ•´ç§å­æ•°æ®é›†æ·±åº¦æ¢ç´¢...")

        start_time = time.time()

        # æ­¥éª¤1: åŠ è½½æ•°æ®
        if not self.load_full_dataset():
            return False

        # æ­¥éª¤2: æ•°æ®ç»“æ„åˆ†æ
        self.analyze_data_structure()

        # æ­¥éª¤3: æ–‡æœ¬ç‰¹å¾åˆ†æ
        self.analyze_text_features_comprehensive()

        # æ­¥éª¤4: å›¾åƒç‰¹å¾åˆ†æ
        self.analyze_image_features_sampled()

        # æ­¥éª¤5: ç”ŸæˆæŠ¥å‘Š
        self.generate_comprehensive_report()

        total_time = time.time() - start_time
        print(f"\nğŸ‰ å®Œæ•´æ•°æ®æ¢ç´¢åˆ†æå®Œæˆï¼æ€»è€—æ—¶: {total_time:.1f}ç§’")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"ğŸ“Š å›¾è¡¨ç›®å½•: {self.charts_dir}")

        return True

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨å®Œæ•´ç§å­æ•°æ®é›†æ¢ç´¢å™¨...")
    print("ğŸ“Š ç›®æ ‡: åˆ†æ40ä¸‡å®Œæ•´ç§å­æ•°æ®é›†")

    # åˆ›å»ºæ¢ç´¢å™¨å®ä¾‹ (ä½¿ç”¨5ä¸‡æ ·æœ¬è¿›è¡Œå›¾åƒåˆ†æ)
    explorer = FullDatasetExplorer(sample_size=50000)

    success = explorer.run_full_exploration()

    if success:
        print("\nâœ… æ•°æ®æ¢ç´¢æˆåŠŸå®Œæˆï¼")
        print("ğŸ“‹ ä¸‹ä¸€æ­¥å»ºè®®:")
        print("1. æŸ¥çœ‹åˆ†ææŠ¥å‘Š: output/full_dataset_analysis/comprehensive_analysis_report.md")
        print("2. æŸ¥çœ‹å¯è§†åŒ–å›¾è¡¨: output/full_dataset_analysis/charts/")
        print("3. åŸºäºåˆ†æç»“æœä¼˜åŒ–Data-Juiceré…ç½®")
        print("4. å¯åŠ¨å¤§è§„æ¨¡æ•°æ®åˆæˆæµç¨‹")
    else:
        print("\nâŒ æ•°æ®æ¢ç´¢å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶å’Œç¯å¢ƒ")

if __name__ == "__main__":
    main()
