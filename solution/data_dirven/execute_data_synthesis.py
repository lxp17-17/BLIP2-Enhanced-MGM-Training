#!/usr/bin/env python3
"""
æ•°æ®åˆæˆæ‰§è¡Œè„šæœ¬ - Phase 1: æ•°æ®åˆæˆè®¡åˆ’
åŸºäºLoRAè®­ç»ƒæˆåŠŸçš„åŸºç¡€ä¸Šï¼Œæ‰§è¡Œé«˜è´¨é‡æ•°æ®åˆæˆ
"""

import os
import sys
import json
import time
import subprocess
from pathlib import Path
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('output/data_synthesis.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class DataSynthesisExecutor:
    def __init__(self):
        self.project_root = Path.cwd()
        self.data_juicer_path = self.project_root / "toolkit" / "data-juicer"
        self.config_path = self.project_root / "solution" / "basic_data_synthesis.yaml"
        self.output_dir = self.project_root / "output" / "processed_data"
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
    def check_prerequisites(self):
        """æ£€æŸ¥æ‰§è¡Œå‰ææ¡ä»¶"""
        logger.info("ğŸ” æ£€æŸ¥æ‰§è¡Œå‰ææ¡ä»¶...")

        # æ£€æŸ¥å®Œæ•´ç§å­æ•°æ® (40ä¸‡æ•°æ®)
        seed_data = "input/pretrain_stage_1/mgm_pretrain_stage_1.jsonl"
        if not os.path.exists(seed_data):
            logger.error(f"âŒ å®Œæ•´ç§å­æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {seed_data}")
            return False
        logger.info(f"âœ… å®Œæ•´ç§å­æ•°æ®æ–‡ä»¶å­˜åœ¨: {seed_data}")

        # æ£€æŸ¥æ•°æ®å¤§å°
        file_size = os.path.getsize(seed_data) / (1024**3)  # GB
        logger.info(f"ğŸ“Š æ•°æ®æ–‡ä»¶å¤§å°: {file_size:.2f}GB")
        
        # æ£€æŸ¥Data-Juicer
        if not self.data_juicer_path.exists():
            logger.error(f"âŒ Data-Juicerè·¯å¾„ä¸å­˜åœ¨: {self.data_juicer_path}")
            return False
        logger.info(f"âœ… Data-Juicerè·¯å¾„å­˜åœ¨: {self.data_juicer_path}")
        
        # æ£€æŸ¥é…ç½®æ–‡ä»¶
        if not self.config_path.exists():
            logger.error(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {self.config_path}")
            return False
        logger.info(f"âœ… é…ç½®æ–‡ä»¶å­˜åœ¨: {self.config_path}")
        
        # æ£€æŸ¥ç³»ç»Ÿèµ„æº
        import psutil
        memory_gb = psutil.virtual_memory().total / (1024**3)
        available_gb = psutil.virtual_memory().available / (1024**3)
        logger.info(f"ğŸ“Š ç³»ç»Ÿå†…å­˜: {memory_gb:.1f}GB (å¯ç”¨: {available_gb:.1f}GB)")

        if available_gb < 40:
            logger.warning("âš ï¸ å¯ç”¨å†…å­˜å¯èƒ½ä¸è¶³ï¼Œå¤„ç†40ä¸‡æ•°æ®å»ºè®®è‡³å°‘40GBå¯ç”¨å†…å­˜")

        # é¢„ä¼°å¤„ç†æ—¶é—´
        logger.info("â±ï¸ é¢„ä¼°å¤„ç†æ—¶é—´: 10-12å°æ—¶ (40ä¸‡æ•°æ®)")
        logger.info("ğŸ¯ ç›®æ ‡: æ–‡æœ¬é•¿åº¦ 8.78â†’15-20è¯, è¯æ±‡å¤šæ ·æ€§ 0.0714â†’0.200+")

        return True
    
    def execute_data_processing(self):
        """æ‰§è¡Œæ•°æ®å¤„ç†"""
        logger.info("ğŸš€ å¼€å§‹æ‰§è¡ŒData-Juiceræ•°æ®å¤„ç†...")
        
        # æ„å»ºå‘½ä»¤ - ä½¿ç”¨condaç¯å¢ƒçš„Python
        conda_python = "/home/robot/lhp/miniconda3/envs/Syn0625/bin/python"
        cmd = [
            conda_python,  # ä½¿ç”¨condaç¯å¢ƒPython
            "-m", "data_juicer.tools.process_data",
            "--config", str(self.config_path),
            "--work_dir", str(self.output_dir)
        ]
        
        logger.info(f"ğŸ“ æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        try:
            # åˆ‡æ¢åˆ°data-juicerç›®å½•
            os.chdir(self.data_juicer_path)
            
            # æ‰§è¡Œå¤„ç† (40ä¸‡æ•°æ®éœ€è¦æ›´é•¿æ—¶é—´)
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=43200  # 12å°æ—¶è¶…æ—¶ (40ä¸‡æ•°æ®)
            )
            
            # è®°å½•ç»“æŸæ—¶é—´
            end_time = time.time()
            duration = end_time - start_time
            
            if result.returncode == 0:
                logger.info(f"âœ… æ•°æ®å¤„ç†æˆåŠŸå®Œæˆï¼Œè€—æ—¶: {duration:.1f}ç§’")
                logger.info("ğŸ“Š å¤„ç†è¾“å‡º:")
                logger.info(result.stdout)
                return True
            else:
                logger.error(f"âŒ æ•°æ®å¤„ç†å¤±è´¥ï¼Œè¿”å›ç : {result.returncode}")
                logger.error("é”™è¯¯è¾“å‡º:")
                logger.error(result.stderr)
                return False
                
        except subprocess.TimeoutExpired:
            logger.error("âŒ æ•°æ®å¤„ç†è¶…æ—¶ï¼ˆ12å°æ—¶ï¼‰")
            return False
        except Exception as e:
            logger.error(f"âŒ æ•°æ®å¤„ç†å¼‚å¸¸: {e}")
            return False
        finally:
            # åˆ‡æ¢å›åŸç›®å½•
            os.chdir(self.project_root)
    
    def validate_output(self):
        """éªŒè¯è¾“å‡ºæ•°æ®è´¨é‡"""
        logger.info("ğŸ” éªŒè¯è¾“å‡ºæ•°æ®è´¨é‡...")
        
        output_file = self.output_dir / "basic_enhanced_data.jsonl"
        if not output_file.exists():
            logger.error(f"âŒ è¾“å‡ºæ–‡ä»¶ä¸å­˜åœ¨: {output_file}")
            return False
        
        # ç»Ÿè®¡è¾“å‡ºæ•°æ®
        try:
            with open(output_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                sample_count = len(lines)
            
            logger.info(f"ğŸ“Š è¾“å‡ºæ ·æœ¬æ•°: {sample_count}")
            
            # åˆ†æå‰å‡ ä¸ªæ ·æœ¬
            if sample_count > 0:
                with open(output_file, 'r', encoding='utf-8') as f:
                    sample = json.loads(f.readline())
                    
                logger.info("ğŸ“ æ ·æœ¬å­—æ®µ:")
                for key in sample.keys():
                    logger.info(f"  - {key}")
                
                # æ£€æŸ¥æ–‡æœ¬é•¿åº¦æ”¹å–„
                if 'enhanced_text' in sample:
                    enhanced_length = len(sample['enhanced_text'].split())
                    original_length = len(sample.get('text', '').split())
                    logger.info(f"ğŸ“ˆ æ–‡æœ¬é•¿åº¦æ”¹å–„: {original_length} â†’ {enhanced_length} è¯")
                
            return sample_count > 0
            
        except Exception as e:
            logger.error(f"âŒ éªŒè¯è¾“å‡ºæ•°æ®æ—¶å‡ºé”™: {e}")
            return False
    
    def generate_summary_report(self):
        """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
        logger.info("ğŸ“ ç”Ÿæˆæ•°æ®åˆæˆæ€»ç»“æŠ¥å‘Š...")
        
        report_path = self.output_dir / "synthesis_report.md"
        
        # æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
        input_file = "input/pretrain_stage_1/mgm_pretrain_stage_1.jsonl"
        output_file = self.output_dir / "basic_enhanced_data.jsonl"
        
        input_count = 0
        output_count = 0
        
        if os.path.exists(input_file):
            with open(input_file, 'r') as f:
                input_count = len(f.readlines())
        
        if output_file.exists():
            with open(output_file, 'r') as f:
                output_count = len(f.readlines())
        
        # ç”ŸæˆæŠ¥å‘Š
        report_content = f"""# æ•°æ®åˆæˆæ‰§è¡ŒæŠ¥å‘Š

## æ‰§è¡Œä¿¡æ¯
- **æ‰§è¡Œæ—¶é—´**: {time.strftime('%Y-%m-%d %H:%M:%S')}
- **é…ç½®æ–‡ä»¶**: {self.config_path.name}
- **å¤„ç†ç­–ç•¥**: åŸºäºæ•°æ®æ¢ç´¢ç»“æœçš„å®šåˆ¶åŒ–ç­–ç•¥

## æ•°æ®ç»Ÿè®¡
- **è¾“å…¥æ ·æœ¬æ•°**: {input_count:,}
- **è¾“å‡ºæ ·æœ¬æ•°**: {output_count:,}
- **æ•°æ®å¢é•¿ç‡**: {(output_count/input_count*100-100):.1f}% (å¦‚æœ>0è¡¨ç¤ºæ•°æ®å¢å¼ºæˆåŠŸ)

## å¤„ç†ç­–ç•¥
1. **æ–‡æœ¬ä¸°å¯ŒåŒ–**: ä½¿ç”¨BLIP2å¤šæ¨¡å‹ç”Ÿæˆè¯¦ç»†æè¿°
2. **è¯æ±‡å¤šæ ·åŒ–**: å¤šè§’åº¦å›¾åƒæ ‡ç­¾ç”Ÿæˆ
3. **å†…å®¹å¢å¼º**: æ–‡æœ¬èåˆå’Œé—®ç­”ç”Ÿæˆ
4. **è´¨é‡æ§åˆ¶**: å¤šå±‚è¿‡æ»¤ç¡®ä¿æ•°æ®è´¨é‡

## ä¸‹ä¸€æ­¥è®¡åˆ’
1. éªŒè¯åˆæˆæ•°æ®è´¨é‡
2. å‡†å¤‡å®Œæ•´MGMè®­ç»ƒ
3. å¯¹æ¯”è®­ç»ƒæ•ˆæœ

---
*æŠ¥å‘Šç”±æ•°æ®åˆæˆæ‰§è¡Œå™¨è‡ªåŠ¨ç”Ÿæˆ*
"""
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        logger.info(f"âœ… æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
    
    def run_full_synthesis(self):
        """è¿è¡Œå®Œæ•´çš„æ•°æ®åˆæˆæµç¨‹"""
        logger.info("ğŸ¯ å¼€å§‹Phase 1: æ•°æ®åˆæˆè®¡åˆ’æ‰§è¡Œ")
        
        # æ­¥éª¤1: æ£€æŸ¥å‰ææ¡ä»¶
        if not self.check_prerequisites():
            logger.error("âŒ å‰ææ¡ä»¶æ£€æŸ¥å¤±è´¥ï¼Œç»ˆæ­¢æ‰§è¡Œ")
            return False
        
        # æ­¥éª¤2: æ‰§è¡Œæ•°æ®å¤„ç†
        if not self.execute_data_processing():
            logger.error("âŒ æ•°æ®å¤„ç†å¤±è´¥ï¼Œç»ˆæ­¢æ‰§è¡Œ")
            return False
        
        # æ­¥éª¤3: éªŒè¯è¾“å‡º
        if not self.validate_output():
            logger.error("âŒ è¾“å‡ºéªŒè¯å¤±è´¥")
            return False
        
        # æ­¥éª¤4: ç”ŸæˆæŠ¥å‘Š
        self.generate_summary_report()
        
        logger.info("ğŸ‰ Phase 1: æ•°æ®åˆæˆè®¡åˆ’æ‰§è¡Œå®Œæˆï¼")
        logger.info("ğŸ“‹ ä¸‹ä¸€æ­¥: å‡†å¤‡å®Œæ•´MGMè®­ç»ƒ")
        
        return True

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨æ•°æ®åˆæˆæ‰§è¡Œå™¨...")
    
    executor = DataSynthesisExecutor()
    success = executor.run_full_synthesis()
    
    if success:
        print("\nâœ… æ•°æ®åˆæˆæˆåŠŸå®Œæˆï¼")
        print("ğŸ“ è¾“å‡ºç›®å½•: output/processed_data/")
        print("ğŸ“ æŸ¥çœ‹æŠ¥å‘Š: output/processed_data/synthesis_report.md")
        print("ğŸ“Š æŸ¥çœ‹æ—¥å¿—: output/data_synthesis.log")
    else:
        print("\nâŒ æ•°æ®åˆæˆå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")
        sys.exit(1)

if __name__ == "__main__":
    main()
