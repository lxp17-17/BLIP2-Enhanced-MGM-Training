#!/usr/bin/env python3
"""
10kåŸºçº¿æ•°æ®åˆ†æè„šæœ¬
åˆ†ææ•°æ®æ ¼å¼ã€å†…å®¹ç‰¹å¾ï¼Œä¸ºæ•°æ®å¤„ç†ç­–ç•¥æä¾›å‚è€ƒ
"""

import json
import os
from collections import Counter

def analyze_10k_data():
    """åˆ†æ10kæ•°æ®é›†"""
    data_path = "input/pretrain_stage_1_10k/mgm_pretrain_stage_1_10k.jsonl"
    
    print("=== 10KåŸºçº¿æ•°æ®åˆ†æ ===\n")
    
    # è¯»å–æ‰€æœ‰æ•°æ®
    data_samples = []
    with open(data_path, 'r', encoding='utf-8') as f:
        for line in f:
            data_samples.append(json.loads(line.strip()))
    
    print(f"ğŸ“Š æ•°æ®é‡: {len(data_samples)} æ¡è®°å½•")
    
    # åˆ†ææ–‡æœ¬é•¿åº¦
    text_lengths = [len(sample['text']) for sample in data_samples]
    print(f"ğŸ“ æ–‡æœ¬é•¿åº¦ç»Ÿè®¡:")
    print(f"   - å¹³å‡é•¿åº¦: {sum(text_lengths)/len(text_lengths):.1f} å­—ç¬¦")
    print(f"   - æœ€çŸ­: {min(text_lengths)} å­—ç¬¦") 
    print(f"   - æœ€é•¿: {max(text_lengths)} å­—ç¬¦")
    
    # åˆ†ææ–‡æœ¬å†…å®¹ç‰¹å¾
    print(f"\nğŸ“‹ æ–‡æœ¬å†…å®¹åˆ†æ:")
    
    # ç»Ÿè®¡æ˜¯å¦åŒ…å«ç‰¹æ®Šæ ‡è®°
    dj_image_count = sum(1 for sample in data_samples if '<__dj__image>' in sample['text'])
    eoc_count = sum(1 for sample in data_samples if '<|__dj__eoc|>' in sample['text'])
    
    print(f"   - åŒ…å« <__dj__image> æ ‡è®°: {dj_image_count} ({dj_image_count/len(data_samples)*100:.1f}%)")
    print(f"   - åŒ…å« <|__dj__eoc|> æ ‡è®°: {eoc_count} ({eoc_count/len(data_samples)*100:.1f}%)")
    
    # åˆ†æå›¾åƒä¿¡æ¯
    print(f"\nğŸ–¼ï¸ å›¾åƒä¿¡æ¯åˆ†æ:")
    
    image_counts = [len(sample['images']) for sample in data_samples]
    print(f"   - æ¯æ¡è®°å½•å›¾åƒæ•°é‡: æœ€å°‘{min(image_counts)}, æœ€å¤š{max(image_counts)}, å¹³å‡{sum(image_counts)/len(image_counts):.1f}")
    
    # æ£€æŸ¥å›¾åƒæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    existing_images = 0
    missing_images = 0
    
    for sample in data_samples[:100]:  # æ£€æŸ¥å‰100ä¸ªæ ·æœ¬
        for img_path in sample['images']:
            full_path = f"input/pretrain_stage_1_10k/{img_path}"
            if os.path.exists(full_path):
                existing_images += 1
            else:
                missing_images += 1
    
    print(f"   - å›¾åƒæ–‡ä»¶æ£€æŸ¥ (å‰100æ ·æœ¬): å­˜åœ¨{existing_images}, ç¼ºå¤±{missing_images}")
    
    # åˆ†ææ–‡æœ¬å†…å®¹æ¨¡å¼
    print(f"\nğŸ“ˆ æ–‡æœ¬å†…å®¹æ¨¡å¼:")
    
    # ç§»é™¤ç‰¹æ®Šæ ‡è®°åçš„çº¯æ–‡æœ¬
    clean_texts = []
    for sample in data_samples:
        text = sample['text']
        text = text.replace('<__dj__image>', '').replace('<|__dj__eoc|>', '').strip()
        clean_texts.append(text)
    
    # ç»Ÿè®¡å¸¸è§è¯æ±‡
    all_words = []
    for text in clean_texts:
        words = text.lower().split()
        all_words.extend(words)
    
    word_freq = Counter(all_words)
    print(f"   - æ€»è¯æ±‡æ•°: {len(word_freq)}")
    print(f"   - æœ€å¸¸è§çš„10ä¸ªè¯:")
    for word, count in word_freq.most_common(10):
        print(f"     {word}: {count}æ¬¡")
    
    # å±•ç¤ºå‡ ä¸ªä»£è¡¨æ€§æ ·æœ¬
    print(f"\nğŸ“ ä»£è¡¨æ€§æ ·æœ¬:")
    for i, sample in enumerate(data_samples[:5]):
        clean_text = sample['text'].replace('<__dj__image>', '').replace('<|__dj__eoc|>', '').strip()
        print(f"   {i+1}. ID: {sample['id']}")
        print(f"      æ–‡æœ¬: {clean_text}")
        print(f"      å›¾åƒ: {sample['images'][0]}")
        print()
    
    # æ•°æ®è´¨é‡è¯„ä¼°
    print(f"ğŸ” æ•°æ®è´¨é‡è¯„ä¼°:")
    
    # æ£€æŸ¥å¼‚å¸¸çŸ­æ–‡æœ¬
    short_texts = [sample for sample in data_samples if len(sample['text'].replace('<__dj__image>', '').replace('<|__dj__eoc|>', '').strip()) < 10]
    print(f"   - å¼‚å¸¸çŸ­æ–‡æœ¬ (<10å­—ç¬¦): {len(short_texts)} æ¡ ({len(short_texts)/len(data_samples)*100:.1f}%)")
    
    # æ£€æŸ¥å¯èƒ½çš„é‡å¤
    text_hashes = set()
    duplicates = 0
    for sample in data_samples:
        text_hash = hash(sample['text'])
        if text_hash in text_hashes:
            duplicates += 1
        text_hashes.add(text_hash)
    
    print(f"   - å¯èƒ½çš„é‡å¤æ–‡æœ¬: {duplicates} æ¡ ({duplicates/len(data_samples)*100:.1f}%)")
    
    print(f"\nâœ… æ•°æ®åˆ†æå®Œæˆ!")
    print(f"\nğŸ’¡ å»ºè®®çš„æ•°æ®å¤„ç†ç­–ç•¥:")
    print(f"   1. æ•°æ®è´¨é‡å¾ˆå¥½ï¼Œæ ¼å¼ç»Ÿä¸€")
    print(f"   2. å¯ä»¥å°è¯•æ”¹è¿›æè¿°è´¨é‡ï¼ˆå½“å‰æè¿°è¾ƒç®€å•ï¼‰")
    print(f"   3. å¯ä»¥è€ƒè™‘æ•°æ®å¢å¼ºæŠ€æœ¯")
    print(f"   4. ç‰¹æ®Šæ ‡è®°æ ¼å¼å·²æ ‡å‡†åŒ–ï¼Œä¾¿äºå¤„ç†")

if __name__ == "__main__":
    analyze_10k_data()