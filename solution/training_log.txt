=== [æ­¥éª¤1] éªŒè¯è®­ç»ƒå‚æ•°é…ç½® ===
=== [æ­¥éª¤2] æ•°æ®é¢„å¤„ç† ===
æ­£åœ¨å¯¹æ•°æ®é›†è¿›è¡Œé‡‡æ ·ï¼Œæœ€å¤§æ ·æœ¬æ•°ï¼š200000
2025-06-26 01:08:44.662 | INFO     | __main__:main:14 - Total number of samples in the input dataset: 10000
2025-06-26 01:08:44.662 | INFO     | __main__:main:19 - Keep all samples in the input dataset.
æ­£åœ¨å°†DJæ ¼å¼è½¬æ¢ä¸ºLLaVAæ ¼å¼ï¼š../input/pretrain_stage_1_10k/mgm_pretrain_stage_1_10k.jsonl-200k.jsonl -> ../input/pretrain_stage_1_10k/mgm_pretrain_stage_1_10k.jsonl-200k.jsonl.json
python: can't open file '/home/robot/lhp/projects/0625TCSyn/dj_synth_challenge/toolkit/data-juicer/tools/multimodal/data_juicer_format_to_target_format/dj_to_llava.py': [Errno 2] No such file or directory
=== [æ­¥éª¤3] å¼€å§‹æ¨¡å‹è®­ç»ƒ ===
é¢„è®­ç»ƒæ¨¡å‹å°†ä¿å­˜åˆ°ï¼š/home/robot/lhp/projects/0625TCSyn/dj_synth_challenge/toolkit/../output/training_dirs/MGM-2B-Pretrain-default
--- [é˜¶æ®µ1] å¼€å§‹é¢„è®­ç»ƒï¼ˆå­¦ä¹ å›¾åƒ-æ–‡æœ¬åŸºç¡€å¯¹é½ï¼‰ ---
[2025-06-26 01:08:45,365] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-26 01:08:46,456] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-06-26 01:08:46,459] [WARNING] [runner.py:220:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-06-26 01:08:46,459] [INFO] [runner.py:610:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None /home/robot/lhp/projects/0625TCSyn/dj_synth_challenge/toolkit/training/mgm/train/train_mem.py --deepspeed /home/robot/lhp/projects/0625TCSyn/dj_synth_challenge/toolkit/training/scripts/zero2_offload.json  
[2025-06-26 01:08:47,184] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-26 01:08:48,200] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-06-26 01:08:48,203] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0]}
[2025-06-26 01:08:48,203] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=1, node_rank=0
[2025-06-26 01:08:48,203] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2025-06-26 01:08:48,203] [INFO] [launch.py:164:main] dist_world_size=1
[2025-06-26 01:08:48,203] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
[2025-06-26 01:08:48,204] [INFO] [launch.py:256:main] process 160986 spawned with command: ['/usr/bin/python3', '-u', '/home/robot/lhp/projects/0625TCSyn/dj_synth_challenge/toolkit/training/mgm/train/train_mem.py', '--local_rank=0', '--deepspeed', '/home/robot/lhp/projects/0625TCSyn/dj_synth_challenge/toolkit/training/scripts/zero2_offload.json', ' ']
Traceback (most recent call last):
  File "/home/robot/lhp/projects/0625TCSyn/dj_synth_challenge/toolkit/training/mgm/train/train_mem.py", line 1, in <module>
    from mgm.train.train import train
ModuleNotFoundError: No module named 'mgm'
[2025-06-26 01:08:49,205] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 160986
[2025-06-26 01:08:49,205] [ERROR] [launch.py:325:sigkill_handler] ['/usr/bin/python3', '-u', '/home/robot/lhp/projects/0625TCSyn/dj_synth_challenge/toolkit/training/mgm/train/train_mem.py', '--local_rank=0', '--deepspeed', '/home/robot/lhp/projects/0625TCSyn/dj_synth_challenge/toolkit/training/scripts/zero2_offload.json', ' '] exits with return code = 1
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 96: --model_name_or_path: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 97: --version: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 98: --data_path: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 99: --image_folder: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 100: --vision_tower: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 101: --vision_tower_aux: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 102: --mm_projector_type: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 103: --tune_mm_mlp_adapter: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 104: --mm_vision_select_layer: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 105: --mm_use_im_start_end: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 106: --mm_use_im_patch_token: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 107: --image_size_aux: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 108: --bf16: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 109: --output_dir: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 110: --num_train_epochs: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 111: --per_device_train_batch_size: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 112: --per_device_eval_batch_size: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 113: --gradient_accumulation_steps: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 114: --evaluation_strategy: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 115: --save_strategy: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 116: --save_steps: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 117: --save_total_limit: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 118: --learning_rate: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 119: --weight_decay: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 120: --warmup_ratio: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 121: --lr_scheduler_type: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 122: --logging_steps: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 123: --tf32: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 124: --model_max_length: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 125: --gradient_checkpointing: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 126: --dataloader_num_workers: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 127: --lazy_preprocess: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 128: --report_to: æœªæ‰¾åˆ°å‘½ä»¤
--- [é˜¶æ®µ2] å¼€å§‹æŒ‡ä»¤å¾®è°ƒï¼ˆå¢å¼ºä»»åŠ¡ç‰¹å®šèƒ½åŠ›ï¼‰ ---
å¾®è°ƒæ¨¡å‹å°†ä¿å­˜åˆ°ï¼š/home/robot/lhp/projects/0625TCSyn/dj_synth_challenge/toolkit/../output/training_dirs/MGM-2B-Finetune-default
[2025-06-26 01:08:51,469] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-26 01:08:52,604] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-06-26 01:08:52,607] [WARNING] [runner.py:220:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-06-26 01:08:52,608] [INFO] [runner.py:610:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None /home/robot/lhp/projects/0625TCSyn/dj_synth_challenge/toolkit/training/mgm/train/train_mem.py --deepspeed /home/robot/lhp/projects/0625TCSyn/dj_synth_challenge/toolkit/training/scripts/zero2_offload.json  
[2025-06-26 01:08:53,304] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-26 01:08:54,325] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-06-26 01:08:54,328] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0]}
[2025-06-26 01:08:54,328] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=1, node_rank=0
[2025-06-26 01:08:54,328] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2025-06-26 01:08:54,328] [INFO] [launch.py:164:main] dist_world_size=1
[2025-06-26 01:08:54,328] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
[2025-06-26 01:08:54,328] [INFO] [launch.py:256:main] process 161396 spawned with command: ['/usr/bin/python3', '-u', '/home/robot/lhp/projects/0625TCSyn/dj_synth_challenge/toolkit/training/mgm/train/train_mem.py', '--local_rank=0', '--deepspeed', '/home/robot/lhp/projects/0625TCSyn/dj_synth_challenge/toolkit/training/scripts/zero2_offload.json', ' ']
Traceback (most recent call last):
  File "/home/robot/lhp/projects/0625TCSyn/dj_synth_challenge/toolkit/training/mgm/train/train_mem.py", line 1, in <module>
    from mgm.train.train import train
ModuleNotFoundError: No module named 'mgm'
[2025-06-26 01:08:55,328] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 161396
[2025-06-26 01:08:55,329] [ERROR] [launch.py:325:sigkill_handler] ['/usr/bin/python3', '-u', '/home/robot/lhp/projects/0625TCSyn/dj_synth_challenge/toolkit/training/mgm/train/train_mem.py', '--local_rank=0', '--deepspeed', '/home/robot/lhp/projects/0625TCSyn/dj_synth_challenge/toolkit/training/scripts/zero2_offload.json', ' '] exits with return code = 1
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 139: --model_name_or_path: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 140: --version: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 141: --data_path: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 142: --image_folder: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 143: --vision_tower: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 144: --vision_tower_aux: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 145: --pretrain_mm_mlp_adapter: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 146: --mm_projector_type: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 147: --mm_vision_select_layer: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 148: --mm_use_im_start_end: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 149: --mm_use_im_patch_token: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 150: --image_aspect_ratio: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 151: --group_by_modality_length: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 152: --image_size_aux: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 153: --bf16: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 154: --output_dir: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 155: --num_train_epochs: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 156: --per_device_train_batch_size: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 157: --per_device_eval_batch_size: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 158: --gradient_accumulation_steps: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 159: --evaluation_strategy: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 160: --save_strategy: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 161: --save_steps: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 162: --save_total_limit: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 163: --learning_rate: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 164: --weight_decay: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 165: --warmup_ratio: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 166: --lr_scheduler_type: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 167: --logging_steps: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 168: --tf32: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 169: --model_max_length: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 170: --gradient_checkpointing: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 171: --dataloader_num_workers: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 172: --lazy_preprocess: æœªæ‰¾åˆ°å‘½ä»¤
train_mgm_2b_stage_1_10k_baseline.sh: è¡Œ 173: --report_to: æœªæ‰¾åˆ°å‘½ä»¤
=== [æ­¥éª¤4] å¼€å§‹æ¨¡å‹è¯„ä¼° ===
æ­£åœ¨TextVQAæ•°æ®é›†ä¸Šè¿›è¡Œæ¨ç†è¯„ä¼°...
/home/robot/lhp/miniconda3/envs/Syn0625/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
[2025-06-26 01:08:58,054] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-26 01:08:58,908] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
Traceback (most recent call last):
  File "/home/robot/lhp/miniconda3/envs/Syn0625/lib/python3.10/runpy.py", line 187, in _run_module_as_main
    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)
  File "/home/robot/lhp/miniconda3/envs/Syn0625/lib/python3.10/runpy.py", line 110, in _get_module_details
    __import__(pkg_name)
  File "/home/robot/lhp/projects/0625TCSyn/dj_synth_challenge/toolkit/training/mgm/__init__.py", line 1, in <module>
    from .model import MGMLlamaForCausalLM
  File "/home/robot/lhp/projects/0625TCSyn/dj_synth_challenge/toolkit/training/mgm/model/__init__.py", line 1, in <module>
    from .language_model.mgm_llama import MGMLlamaForCausalLM
  File "/home/robot/lhp/projects/0625TCSyn/dj_synth_challenge/toolkit/training/mgm/model/language_model/mgm_llama.py", line 31, in <module>
    from mgm.model.mgm_arch import MGMMetaModel, MGMMetaForCausalLM
  File "/home/robot/lhp/projects/0625TCSyn/dj_synth_challenge/toolkit/training/mgm/model/mgm_arch.py", line 31, in <module>
    from .multimodal_encoder.builder import build_vision_tower, build_vision_tower_aux
  File "/home/robot/lhp/projects/0625TCSyn/dj_synth_challenge/toolkit/training/mgm/model/multimodal_encoder/builder.py", line 3, in <module>
    from .eva_encoder import EVAVisionTower
  File "/home/robot/lhp/projects/0625TCSyn/dj_synth_challenge/toolkit/training/mgm/model/multimodal_encoder/eva_encoder.py", line 15, in <module>
    from timm.models.layers import drop_path, to_2tuple, trunc_normal_
ModuleNotFoundError: No module named 'timm'
/home/robot/lhp/projects/0625TCSyn/dj_synth_challenge/toolkit/eval/textvqa.sh: è¡Œ 28: /home/robot/lhp/projects/0625TCSyn/dj_synth_challenge/toolkit/eval/../../output/eval_results/MGM-2B-Finetune-default/textvqa//bm1.jsonl: æ²¡æœ‰é‚£ä¸ªæ–‡ä»¶æˆ–ç›®å½•
/home/robot/lhp/projects/0625TCSyn/dj_synth_challenge/toolkit/eval/textvqa.sh: è¡Œ 32: /home/robot/lhp/projects/0625TCSyn/dj_synth_challenge/toolkit/eval/../../output/eval_results/MGM-2B-Finetune-default/textvqa//bm1.jsonl: æ²¡æœ‰é‚£ä¸ªæ–‡ä»¶æˆ–ç›®å½•
æ­£åœ¨MMBenchæ•°æ®é›†ä¸Šè¿›è¡Œæ¨ç†è¯„ä¼°...
/home/robot/lhp/miniconda3/envs/Syn0625/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
[2025-06-26 01:09:00,321] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-26 01:09:01,166] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
Traceback (most recent call last):
  File "/home/robot/lhp/miniconda3/envs/Syn0625/lib/python3.10/runpy.py", line 187, in _run_module_as_main
    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)
  File "/home/robot/lhp/miniconda3/envs/Syn0625/lib/python3.10/runpy.py", line 110, in _get_module_details
    __import__(pkg_name)
  File "/home/robot/lhp/projects/0625TCSyn/dj_synth_challenge/toolkit/training/mgm/__init__.py", line 1, in <module>
    from .model import MGMLlamaForCausalLM
  File "/home/robot/lhp/projects/0625TCSyn/dj_synth_challenge/toolkit/training/mgm/model/__init__.py", line 1, in <module>
    from .language_model.mgm_llama import MGMLlamaForCausalLM
  File "/home/robot/lhp/projects/0625TCSyn/dj_synth_challenge/toolkit/training/mgm/model/language_model/mgm_llama.py", line 31, in <module>
    from mgm.model.mgm_arch import MGMMetaModel, MGMMetaForCausalLM
  File "/home/robot/lhp/projects/0625TCSyn/dj_synth_challenge/toolkit/training/mgm/model/mgm_arch.py", line 31, in <module>
    from .multimodal_encoder.builder import build_vision_tower, build_vision_tower_aux
  File "/home/robot/lhp/projects/0625TCSyn/dj_synth_challenge/toolkit/training/mgm/model/multimodal_encoder/builder.py", line 3, in <module>
    from .eva_encoder import EVAVisionTower
  File "/home/robot/lhp/projects/0625TCSyn/dj_synth_challenge/toolkit/training/mgm/model/multimodal_encoder/eva_encoder.py", line 15, in <module>
    from timm.models.layers import drop_path, to_2tuple, trunc_normal_
ModuleNotFoundError: No module named 'timm'
Traceback (most recent call last):
  File "/home/robot/lhp/projects/0625TCSyn/dj_synth_challenge/toolkit/eval/../training/scripts/convert_mmbench_for_submission.py", line 23, in <module>
    for pred in open(os.path.join(args.result_dir, f"{args.experiment}.jsonl")):
FileNotFoundError: [Errno 2] No such file or directory: '/home/robot/lhp/projects/0625TCSyn/dj_synth_challenge/toolkit/eval/../../output/eval_results/MGM-2B-Finetune-default/mmbench/answers/mmbench_dev_20230712/MGM-2B-Finetune-default.jsonl'
=== [æ­¥éª¤5] æ•´ç†è¾“å‡ºæ–‡ä»¶ ===
ğŸ‰ è®­ç»ƒå’Œæ¨ç†å…¨éƒ¨å®Œæˆï¼
ğŸ“ è®­ç»ƒæ¨¡å‹ä¿å­˜ä½ç½®ï¼šoutput/training_dirs/MGM-2B-Finetune-default
ğŸ“Š æ¨ç†ç»“æœä¿å­˜ä½ç½®ï¼šoutput/eval_results/MGM-2B-Finetune-default
ğŸ“‹ æ—¥å¿—æ–‡ä»¶ä½ç½®ï¼š
   - é¢„è®­ç»ƒæ—¥å¿—ï¼šoutput/training_dirs/MGM-2B-Pretrain-default/pretrain.log
   - å¾®è°ƒæ—¥å¿—ï¼šoutput/training_dirs/MGM-2B-Finetune-default/finetuning.log
