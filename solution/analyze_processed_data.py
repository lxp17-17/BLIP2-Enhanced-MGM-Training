#!/usr/bin/env python3
"""
å¤„ç†åæ•°æ®è´¨é‡åˆ†æè„šæœ¬
å¯¹æ¯”å¤„ç†å‰åçš„æ•°æ®è´¨é‡æ”¹å–„æƒ…å†µ
"""

import json
import numpy as np
from collections import Counter
import re

def analyze_processed_data():
    """åˆ†æå¤„ç†åçš„æ•°æ®è´¨é‡"""
    print("ğŸ” åˆ†æå¤„ç†åçš„æ•°æ®è´¨é‡...")
    
    # è¯»å–å¤„ç†åçš„æ•°æ®
    processed_data = []
    with open('output/processed_data/basic_enhanced_data.jsonl', 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                processed_data.append(json.loads(line.strip()))
    
    print(f"ğŸ“Š å¤„ç†åæ•°æ®é‡: {len(processed_data):,}")
    
    # åˆ†ææ–‡æœ¬ç‰¹å¾
    texts = []
    for item in processed_data:
        text = item.get('text', '')
        # æ¸…ç†ç‰¹æ®Šæ ‡è®°
        clean_text = text.replace('<__dj__image>', '').replace('<|__dj__eoc|>', '').strip()
        texts.append(clean_text)
    
    # è®¡ç®—æ–‡æœ¬é•¿åº¦ç»Ÿè®¡
    word_lengths = [len(text.split()) for text in texts if text]
    char_lengths = [len(text) for text in texts if text]
    
    print(f"\nğŸ“ æ–‡æœ¬è´¨é‡åˆ†æ:")
    print(f"  å¹³å‡è¯æ•°: {np.mean(word_lengths):.2f}")
    print(f"  ä¸­ä½æ•°è¯æ•°: {np.median(word_lengths):.2f}")
    print(f"  å¹³å‡å­—ç¬¦æ•°: {np.mean(char_lengths):.2f}")
    print(f"  è¯æ•°èŒƒå›´: {min(word_lengths)} - {max(word_lengths)}")
    
    # è´¨é‡åˆ†å¸ƒ
    empty_texts = len([t for t in texts if not t])
    short_texts = len([l for l in word_lengths if l <= 3])
    long_texts = len([l for l in word_lengths if l >= 50])
    
    print(f"\nğŸ“‹ è´¨é‡åˆ†å¸ƒ:")
    print(f"  ç©ºæ–‡æœ¬: {empty_texts} ({empty_texts/len(texts)*100:.2f}%)")
    print(f"  è¿‡çŸ­æ–‡æœ¬(â‰¤3è¯): {short_texts} ({short_texts/len(word_lengths)*100:.2f}%)")
    print(f"  è¿‡é•¿æ–‡æœ¬(â‰¥50è¯): {long_texts} ({long_texts/len(word_lengths)*100:.2f}%)")
    
    # è¯æ±‡å¤šæ ·æ€§åˆ†æ
    all_words = []
    for text in texts[:10000]:  # é‡‡æ ·åˆ†æ
        words = re.findall(r'\b[a-zA-Z]+\b', text.lower())
        all_words.extend(words)
    
    word_freq = Counter(all_words)
    vocabulary_diversity = len(word_freq) / len(all_words) if all_words else 0
    
    print(f"\nğŸ“ˆ è¯æ±‡å¤šæ ·æ€§ (åŸºäº1ä¸‡æ ·æœ¬):")
    print(f"  æ€»è¯æ±‡æ•°: {len(all_words):,}")
    print(f"  å”¯ä¸€è¯æ±‡æ•°: {len(word_freq):,}")
    print(f"  è¯æ±‡å¤šæ ·æ€§: {vocabulary_diversity:.4f}")
    
    # æ˜¾ç¤ºä¸€äº›æ ·æœ¬
    print(f"\nğŸ“ å¤„ç†åæ ·æœ¬ç¤ºä¾‹:")
    for i, item in enumerate(processed_data[:5]):
        clean_text = item['text'].replace('<__dj__image>', '').replace('<|__dj__eoc|>', '').strip()
        print(f"  {i+1}. {clean_text}")
    
    # å¯¹æ¯”åŸå§‹æ•°æ®
    print(f"\nğŸ“Š ä¸åŸå§‹æ•°æ®å¯¹æ¯”:")
    print(f"  åŸå§‹æ•°æ®: 400,000 â†’ å¤„ç†å: {len(processed_data):,}")
    print(f"  æ•°æ®ä¿ç•™ç‡: {len(processed_data)/400000*100:.1f}%")
    print(f"  åŸå§‹å¹³å‡è¯æ•°: 8.78 â†’ å¤„ç†å: {np.mean(word_lengths):.2f}")
    print(f"  åŸå§‹è¯æ±‡å¤šæ ·æ€§: 0.0714 â†’ å¤„ç†å: {vocabulary_diversity:.4f}")
    
    improvement = (np.mean(word_lengths) - 8.78) / 8.78 * 100
    diversity_improvement = (vocabulary_diversity - 0.0714) / 0.0714 * 100
    
    print(f"\nğŸ¯ æ”¹å–„æ•ˆæœ:")
    print(f"  æ–‡æœ¬é•¿åº¦æ”¹å–„: {improvement:+.1f}%")
    print(f"  è¯æ±‡å¤šæ ·æ€§æ”¹å–„: {diversity_improvement:+.1f}%")
    
    return {
        'total_samples': len(processed_data),
        'avg_word_length': float(np.mean(word_lengths)),
        'vocabulary_diversity': vocabulary_diversity,
        'retention_rate': len(processed_data)/400000*100,
        'improvement_word_length': improvement,
        'improvement_diversity': diversity_improvement
    }

if __name__ == "__main__":
    results = analyze_processed_data()
    print(f"\nâœ… æ•°æ®è´¨é‡åˆ†æå®Œæˆï¼")
    print(f"ğŸ“‹ å»ºè®®ä¸‹ä¸€æ­¥: ä½¿ç”¨å¤„ç†åçš„æ•°æ®è¿›è¡ŒMGMå®Œæ•´è®­ç»ƒ")
